{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 2.9829104091144485,
  "eval_steps": 500,
  "global_step": 3600,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.004142931123770067,
      "grad_norm": 0.44179707765579224,
      "learning_rate": 4.9999764380104073e-05,
      "loss": 2.7508,
      "step": 5
    },
    {
      "epoch": 0.008285862247540134,
      "grad_norm": 0.6124097108840942,
      "learning_rate": 4.9999057524857626e-05,
      "loss": 2.6469,
      "step": 10
    },
    {
      "epoch": 0.012428793371310202,
      "grad_norm": 0.7449294328689575,
      "learning_rate": 4.99978794475846e-05,
      "loss": 2.7644,
      "step": 15
    },
    {
      "epoch": 0.016571724495080268,
      "grad_norm": 0.8082029819488525,
      "learning_rate": 4.999623017049125e-05,
      "loss": 2.7196,
      "step": 20
    },
    {
      "epoch": 0.020714655618850338,
      "grad_norm": 0.9762834310531616,
      "learning_rate": 4.999410972466579e-05,
      "loss": 2.5851,
      "step": 25
    },
    {
      "epoch": 0.024857586742620404,
      "grad_norm": 0.9779495000839233,
      "learning_rate": 4.9991518150077765e-05,
      "loss": 2.4056,
      "step": 30
    },
    {
      "epoch": 0.02900051786639047,
      "grad_norm": 1.1257350444793701,
      "learning_rate": 4.998845549557729e-05,
      "loss": 2.4613,
      "step": 35
    },
    {
      "epoch": 0.033143448990160536,
      "grad_norm": 0.9854388236999512,
      "learning_rate": 4.998492181889415e-05,
      "loss": 2.3673,
      "step": 40
    },
    {
      "epoch": 0.03728638011393061,
      "grad_norm": 0.8634601831436157,
      "learning_rate": 4.998091718663671e-05,
      "loss": 2.3045,
      "step": 45
    },
    {
      "epoch": 0.041429311237700675,
      "grad_norm": 0.7118896245956421,
      "learning_rate": 4.9976441674290644e-05,
      "loss": 2.1754,
      "step": 50
    },
    {
      "epoch": 0.04557224236147074,
      "grad_norm": 0.7024465203285217,
      "learning_rate": 4.9971495366217555e-05,
      "loss": 2.1724,
      "step": 55
    },
    {
      "epoch": 0.04971517348524081,
      "grad_norm": 0.7624793648719788,
      "learning_rate": 4.996607835565331e-05,
      "loss": 2.268,
      "step": 60
    },
    {
      "epoch": 0.05385810460901087,
      "grad_norm": 0.6594180464744568,
      "learning_rate": 4.996019074470635e-05,
      "loss": 2.1484,
      "step": 65
    },
    {
      "epoch": 0.05800103573278094,
      "grad_norm": 0.6219938397407532,
      "learning_rate": 4.9953832644355734e-05,
      "loss": 2.1754,
      "step": 70
    },
    {
      "epoch": 0.06214396685655101,
      "grad_norm": 0.6307247877120972,
      "learning_rate": 4.994700417444908e-05,
      "loss": 2.0963,
      "step": 75
    },
    {
      "epoch": 0.06628689798032107,
      "grad_norm": 0.6310924887657166,
      "learning_rate": 4.9939705463700224e-05,
      "loss": 2.0868,
      "step": 80
    },
    {
      "epoch": 0.07042982910409114,
      "grad_norm": 0.6484065055847168,
      "learning_rate": 4.993193664968691e-05,
      "loss": 2.1158,
      "step": 85
    },
    {
      "epoch": 0.07457276022786122,
      "grad_norm": 0.6122162342071533,
      "learning_rate": 4.992369787884809e-05,
      "loss": 2.0796,
      "step": 90
    },
    {
      "epoch": 0.07871569135163128,
      "grad_norm": 0.6355670690536499,
      "learning_rate": 4.991498930648125e-05,
      "loss": 2.0375,
      "step": 95
    },
    {
      "epoch": 0.08285862247540135,
      "grad_norm": 0.6003580093383789,
      "learning_rate": 4.9905811096739405e-05,
      "loss": 1.9739,
      "step": 100
    },
    {
      "epoch": 0.08700155359917142,
      "grad_norm": 0.6539326906204224,
      "learning_rate": 4.9896163422628076e-05,
      "loss": 1.9888,
      "step": 105
    },
    {
      "epoch": 0.09114448472294148,
      "grad_norm": 0.6084616184234619,
      "learning_rate": 4.988604646600197e-05,
      "loss": 1.914,
      "step": 110
    },
    {
      "epoch": 0.09528741584671155,
      "grad_norm": 0.6236608624458313,
      "learning_rate": 4.987546041756159e-05,
      "loss": 2.0457,
      "step": 115
    },
    {
      "epoch": 0.09943034697048161,
      "grad_norm": 0.7131790518760681,
      "learning_rate": 4.986440547684963e-05,
      "loss": 1.9328,
      "step": 120
    },
    {
      "epoch": 0.10357327809425168,
      "grad_norm": 0.6527144908905029,
      "learning_rate": 4.9852881852247214e-05,
      "loss": 2.0514,
      "step": 125
    },
    {
      "epoch": 0.10771620921802175,
      "grad_norm": 0.6471017599105835,
      "learning_rate": 4.9840889760969943e-05,
      "loss": 1.8987,
      "step": 130
    },
    {
      "epoch": 0.11185914034179181,
      "grad_norm": 0.6412312984466553,
      "learning_rate": 4.982842942906386e-05,
      "loss": 1.9354,
      "step": 135
    },
    {
      "epoch": 0.11600207146556188,
      "grad_norm": 0.6233600378036499,
      "learning_rate": 4.981550109140112e-05,
      "loss": 2.0601,
      "step": 140
    },
    {
      "epoch": 0.12014500258933196,
      "grad_norm": 0.6869566440582275,
      "learning_rate": 4.9802104991675613e-05,
      "loss": 2.0471,
      "step": 145
    },
    {
      "epoch": 0.12428793371310203,
      "grad_norm": 0.6040629148483276,
      "learning_rate": 4.978824138239835e-05,
      "loss": 1.9283,
      "step": 150
    },
    {
      "epoch": 0.12843086483687208,
      "grad_norm": 0.5886946320533752,
      "learning_rate": 4.9773910524892706e-05,
      "loss": 1.9354,
      "step": 155
    },
    {
      "epoch": 0.13257379596064214,
      "grad_norm": 0.6403055787086487,
      "learning_rate": 4.9759112689289495e-05,
      "loss": 2.0053,
      "step": 160
    },
    {
      "epoch": 0.1367167270844122,
      "grad_norm": 0.6695410013198853,
      "learning_rate": 4.9743848154521863e-05,
      "loss": 1.9224,
      "step": 165
    },
    {
      "epoch": 0.14085965820818228,
      "grad_norm": 0.6353699564933777,
      "learning_rate": 4.972811720832008e-05,
      "loss": 1.8791,
      "step": 170
    },
    {
      "epoch": 0.14500258933195237,
      "grad_norm": 0.6583020687103271,
      "learning_rate": 4.9711920147206034e-05,
      "loss": 1.9211,
      "step": 175
    },
    {
      "epoch": 0.14914552045572244,
      "grad_norm": 0.6483100056648254,
      "learning_rate": 4.9695257276487736e-05,
      "loss": 1.9607,
      "step": 180
    },
    {
      "epoch": 0.1532884515794925,
      "grad_norm": 0.7762153148651123,
      "learning_rate": 4.9678128910253485e-05,
      "loss": 1.8913,
      "step": 185
    },
    {
      "epoch": 0.15743138270326257,
      "grad_norm": 0.7066433429718018,
      "learning_rate": 4.966053537136598e-05,
      "loss": 1.7876,
      "step": 190
    },
    {
      "epoch": 0.16157431382703263,
      "grad_norm": 0.7638436555862427,
      "learning_rate": 4.964247699145626e-05,
      "loss": 1.993,
      "step": 195
    },
    {
      "epoch": 0.1657172449508027,
      "grad_norm": 0.746681809425354,
      "learning_rate": 4.962395411091741e-05,
      "loss": 1.9344,
      "step": 200
    },
    {
      "epoch": 0.16986017607457277,
      "grad_norm": 0.7147977948188782,
      "learning_rate": 4.960496707889816e-05,
      "loss": 1.8962,
      "step": 205
    },
    {
      "epoch": 0.17400310719834283,
      "grad_norm": 0.6255520582199097,
      "learning_rate": 4.958551625329631e-05,
      "loss": 1.8537,
      "step": 210
    },
    {
      "epoch": 0.1781460383221129,
      "grad_norm": 0.6772159934043884,
      "learning_rate": 4.9565602000751986e-05,
      "loss": 1.9366,
      "step": 215
    },
    {
      "epoch": 0.18228896944588296,
      "grad_norm": 0.7630934119224548,
      "learning_rate": 4.9545224696640716e-05,
      "loss": 1.8889,
      "step": 220
    },
    {
      "epoch": 0.18643190056965303,
      "grad_norm": 0.6898351907730103,
      "learning_rate": 4.9524384725066355e-05,
      "loss": 1.9186,
      "step": 225
    },
    {
      "epoch": 0.1905748316934231,
      "grad_norm": 0.725645899772644,
      "learning_rate": 4.950308247885387e-05,
      "loss": 1.8788,
      "step": 230
    },
    {
      "epoch": 0.19471776281719316,
      "grad_norm": 0.796207070350647,
      "learning_rate": 4.94813183595419e-05,
      "loss": 1.8691,
      "step": 235
    },
    {
      "epoch": 0.19886069394096323,
      "grad_norm": 0.7457296848297119,
      "learning_rate": 4.945909277737519e-05,
      "loss": 1.8976,
      "step": 240
    },
    {
      "epoch": 0.2030036250647333,
      "grad_norm": 0.6991537809371948,
      "learning_rate": 4.943640615129691e-05,
      "loss": 1.8684,
      "step": 245
    },
    {
      "epoch": 0.20714655618850336,
      "grad_norm": 0.8424758315086365,
      "learning_rate": 4.941325890894069e-05,
      "loss": 1.8871,
      "step": 250
    },
    {
      "epoch": 0.21128948731227343,
      "grad_norm": 0.7068098783493042,
      "learning_rate": 4.93896514866226e-05,
      "loss": 1.8874,
      "step": 255
    },
    {
      "epoch": 0.2154324184360435,
      "grad_norm": 0.8106285333633423,
      "learning_rate": 4.9365584329332905e-05,
      "loss": 1.8045,
      "step": 260
    },
    {
      "epoch": 0.21957534955981356,
      "grad_norm": 0.7830763459205627,
      "learning_rate": 4.9341057890727694e-05,
      "loss": 1.8232,
      "step": 265
    },
    {
      "epoch": 0.22371828068358363,
      "grad_norm": 0.7565127015113831,
      "learning_rate": 4.931607263312032e-05,
      "loss": 1.8008,
      "step": 270
    },
    {
      "epoch": 0.2278612118073537,
      "grad_norm": 0.8077191710472107,
      "learning_rate": 4.92906290274727e-05,
      "loss": 1.921,
      "step": 275
    },
    {
      "epoch": 0.23200414293112376,
      "grad_norm": 0.7512942552566528,
      "learning_rate": 4.92647275533864e-05,
      "loss": 1.8141,
      "step": 280
    },
    {
      "epoch": 0.23614707405489382,
      "grad_norm": 0.8262405395507812,
      "learning_rate": 4.923836869909362e-05,
      "loss": 1.807,
      "step": 285
    },
    {
      "epoch": 0.24029000517866392,
      "grad_norm": 0.8069720268249512,
      "learning_rate": 4.921155296144802e-05,
      "loss": 1.8687,
      "step": 290
    },
    {
      "epoch": 0.24443293630243398,
      "grad_norm": 0.7979764342308044,
      "learning_rate": 4.918428084591529e-05,
      "loss": 1.8202,
      "step": 295
    },
    {
      "epoch": 0.24857586742620405,
      "grad_norm": 0.8584378361701965,
      "learning_rate": 4.915655286656368e-05,
      "loss": 1.8677,
      "step": 300
    },
    {
      "epoch": 0.2527187985499741,
      "grad_norm": 0.8507739901542664,
      "learning_rate": 4.9128369546054274e-05,
      "loss": 1.9504,
      "step": 305
    },
    {
      "epoch": 0.25686172967374415,
      "grad_norm": 0.8513964414596558,
      "learning_rate": 4.9099731415631164e-05,
      "loss": 1.818,
      "step": 310
    },
    {
      "epoch": 0.2610046607975142,
      "grad_norm": 0.8115492463111877,
      "learning_rate": 4.907063901511141e-05,
      "loss": 1.8712,
      "step": 315
    },
    {
      "epoch": 0.2651475919212843,
      "grad_norm": 0.8314273953437805,
      "learning_rate": 4.904109289287487e-05,
      "loss": 1.8685,
      "step": 320
    },
    {
      "epoch": 0.26929052304505435,
      "grad_norm": 0.7747696042060852,
      "learning_rate": 4.9011093605853905e-05,
      "loss": 1.9176,
      "step": 325
    },
    {
      "epoch": 0.2734334541688244,
      "grad_norm": 0.8412547707557678,
      "learning_rate": 4.898064171952281e-05,
      "loss": 1.7882,
      "step": 330
    },
    {
      "epoch": 0.2775763852925945,
      "grad_norm": 0.8369797468185425,
      "learning_rate": 4.894973780788722e-05,
      "loss": 1.8366,
      "step": 335
    },
    {
      "epoch": 0.28171931641636455,
      "grad_norm": 0.8265711069107056,
      "learning_rate": 4.8918382453473244e-05,
      "loss": 1.7023,
      "step": 340
    },
    {
      "epoch": 0.2858622475401347,
      "grad_norm": 0.9463041424751282,
      "learning_rate": 4.888657624731651e-05,
      "loss": 1.8229,
      "step": 345
    },
    {
      "epoch": 0.29000517866390474,
      "grad_norm": 0.8241350054740906,
      "learning_rate": 4.885431978895102e-05,
      "loss": 1.8659,
      "step": 350
    },
    {
      "epoch": 0.2941481097876748,
      "grad_norm": 0.806920051574707,
      "learning_rate": 4.8821613686397834e-05,
      "loss": 1.8665,
      "step": 355
    },
    {
      "epoch": 0.29829104091144487,
      "grad_norm": 0.8662183880805969,
      "learning_rate": 4.8788458556153635e-05,
      "loss": 1.7287,
      "step": 360
    },
    {
      "epoch": 0.30243397203521494,
      "grad_norm": 0.9822763204574585,
      "learning_rate": 4.87548550231791e-05,
      "loss": 1.7505,
      "step": 365
    },
    {
      "epoch": 0.306576903158985,
      "grad_norm": 0.8048286437988281,
      "learning_rate": 4.8720803720887086e-05,
      "loss": 1.8016,
      "step": 370
    },
    {
      "epoch": 0.31071983428275507,
      "grad_norm": 0.846747636795044,
      "learning_rate": 4.868630529113075e-05,
      "loss": 1.8486,
      "step": 375
    },
    {
      "epoch": 0.31486276540652514,
      "grad_norm": 0.8615857362747192,
      "learning_rate": 4.865136038419141e-05,
      "loss": 1.8207,
      "step": 380
    },
    {
      "epoch": 0.3190056965302952,
      "grad_norm": 0.8278902173042297,
      "learning_rate": 4.861596965876628e-05,
      "loss": 1.7794,
      "step": 385
    },
    {
      "epoch": 0.32314862765406527,
      "grad_norm": 0.8730960488319397,
      "learning_rate": 4.8580133781956086e-05,
      "loss": 1.7681,
      "step": 390
    },
    {
      "epoch": 0.32729155877783533,
      "grad_norm": 0.8922234773635864,
      "learning_rate": 4.8543853429252485e-05,
      "loss": 1.8476,
      "step": 395
    },
    {
      "epoch": 0.3314344899016054,
      "grad_norm": 0.9329567551612854,
      "learning_rate": 4.85071292845253e-05,
      "loss": 1.7935,
      "step": 400
    },
    {
      "epoch": 0.33557742102537547,
      "grad_norm": 0.9247706532478333,
      "learning_rate": 4.846996204000967e-05,
      "loss": 1.7976,
      "step": 405
    },
    {
      "epoch": 0.33972035214914553,
      "grad_norm": 0.878152072429657,
      "learning_rate": 4.843235239629296e-05,
      "loss": 1.8371,
      "step": 410
    },
    {
      "epoch": 0.3438632832729156,
      "grad_norm": 1.0400607585906982,
      "learning_rate": 4.839430106230162e-05,
      "loss": 1.7572,
      "step": 415
    },
    {
      "epoch": 0.34800621439668566,
      "grad_norm": 0.9119736552238464,
      "learning_rate": 4.835580875528776e-05,
      "loss": 1.9175,
      "step": 420
    },
    {
      "epoch": 0.35214914552045573,
      "grad_norm": 0.9023603200912476,
      "learning_rate": 4.831687620081563e-05,
      "loss": 1.7793,
      "step": 425
    },
    {
      "epoch": 0.3562920766442258,
      "grad_norm": 0.9400020837783813,
      "learning_rate": 4.8277504132747983e-05,
      "loss": 1.7899,
      "step": 430
    },
    {
      "epoch": 0.36043500776799586,
      "grad_norm": 0.9138267040252686,
      "learning_rate": 4.823769329323226e-05,
      "loss": 1.8115,
      "step": 435
    },
    {
      "epoch": 0.36457793889176593,
      "grad_norm": 0.9571700692176819,
      "learning_rate": 4.819744443268649e-05,
      "loss": 1.8474,
      "step": 440
    },
    {
      "epoch": 0.368720870015536,
      "grad_norm": 0.888776421546936,
      "learning_rate": 4.815675830978528e-05,
      "loss": 1.8357,
      "step": 445
    },
    {
      "epoch": 0.37286380113930606,
      "grad_norm": 0.9137511253356934,
      "learning_rate": 4.8115635691445436e-05,
      "loss": 1.7407,
      "step": 450
    },
    {
      "epoch": 0.3770067322630761,
      "grad_norm": 0.9764573574066162,
      "learning_rate": 4.807407735281151e-05,
      "loss": 1.8387,
      "step": 455
    },
    {
      "epoch": 0.3811496633868462,
      "grad_norm": 0.9742116928100586,
      "learning_rate": 4.8032084077241226e-05,
      "loss": 1.8172,
      "step": 460
    },
    {
      "epoch": 0.38529259451061626,
      "grad_norm": 0.9462059140205383,
      "learning_rate": 4.798965665629068e-05,
      "loss": 1.7617,
      "step": 465
    },
    {
      "epoch": 0.3894355256343863,
      "grad_norm": 0.9200564026832581,
      "learning_rate": 4.7946795889699426e-05,
      "loss": 1.7469,
      "step": 470
    },
    {
      "epoch": 0.3935784567581564,
      "grad_norm": 0.9605972170829773,
      "learning_rate": 4.7903502585375415e-05,
      "loss": 1.7957,
      "step": 475
    },
    {
      "epoch": 0.39772138788192646,
      "grad_norm": 0.91707444190979,
      "learning_rate": 4.785977755937977e-05,
      "loss": 1.8923,
      "step": 480
    },
    {
      "epoch": 0.4018643190056965,
      "grad_norm": 1.054023027420044,
      "learning_rate": 4.781562163591135e-05,
      "loss": 1.7738,
      "step": 485
    },
    {
      "epoch": 0.4060072501294666,
      "grad_norm": 0.9407654404640198,
      "learning_rate": 4.7771035647291306e-05,
      "loss": 1.7788,
      "step": 490
    },
    {
      "epoch": 0.41015018125323666,
      "grad_norm": 0.9498825073242188,
      "learning_rate": 4.77260204339473e-05,
      "loss": 1.8336,
      "step": 495
    },
    {
      "epoch": 0.4142931123770067,
      "grad_norm": 1.0330053567886353,
      "learning_rate": 4.768057684439775e-05,
      "loss": 1.7977,
      "step": 500
    },
    {
      "epoch": 0.4184360435007768,
      "grad_norm": 1.0338982343673706,
      "learning_rate": 4.763470573523573e-05,
      "loss": 1.8472,
      "step": 505
    },
    {
      "epoch": 0.42257897462454685,
      "grad_norm": 0.9359288215637207,
      "learning_rate": 4.7588407971112946e-05,
      "loss": 1.7956,
      "step": 510
    },
    {
      "epoch": 0.4267219057483169,
      "grad_norm": 0.9252786040306091,
      "learning_rate": 4.754168442472332e-05,
      "loss": 1.8032,
      "step": 515
    },
    {
      "epoch": 0.430864836872087,
      "grad_norm": 0.9710973501205444,
      "learning_rate": 4.7494535976786645e-05,
      "loss": 1.7641,
      "step": 520
    },
    {
      "epoch": 0.43500776799585705,
      "grad_norm": 1.092440128326416,
      "learning_rate": 4.7446963516031906e-05,
      "loss": 1.7275,
      "step": 525
    },
    {
      "epoch": 0.4391506991196271,
      "grad_norm": 1.090659499168396,
      "learning_rate": 4.739896793918056e-05,
      "loss": 1.7748,
      "step": 530
    },
    {
      "epoch": 0.4432936302433972,
      "grad_norm": 0.8835646510124207,
      "learning_rate": 4.7350550150929626e-05,
      "loss": 1.8287,
      "step": 535
    },
    {
      "epoch": 0.44743656136716725,
      "grad_norm": 0.9421823024749756,
      "learning_rate": 4.730171106393466e-05,
      "loss": 1.8284,
      "step": 540
    },
    {
      "epoch": 0.4515794924909373,
      "grad_norm": 1.0900866985321045,
      "learning_rate": 4.72524515987925e-05,
      "loss": 1.7661,
      "step": 545
    },
    {
      "epoch": 0.4557224236147074,
      "grad_norm": 1.0482643842697144,
      "learning_rate": 4.7202772684023955e-05,
      "loss": 1.7945,
      "step": 550
    },
    {
      "epoch": 0.45986535473847745,
      "grad_norm": 0.9382815361022949,
      "learning_rate": 4.715267525605627e-05,
      "loss": 1.7017,
      "step": 555
    },
    {
      "epoch": 0.4640082858622475,
      "grad_norm": 0.9874008893966675,
      "learning_rate": 4.710216025920552e-05,
      "loss": 1.7718,
      "step": 560
    },
    {
      "epoch": 0.4681512169860176,
      "grad_norm": 0.9941505789756775,
      "learning_rate": 4.705122864565876e-05,
      "loss": 1.8524,
      "step": 565
    },
    {
      "epoch": 0.47229414810978765,
      "grad_norm": 0.9722546935081482,
      "learning_rate": 4.6999881375456114e-05,
      "loss": 1.8035,
      "step": 570
    },
    {
      "epoch": 0.47643707923355777,
      "grad_norm": 1.0302674770355225,
      "learning_rate": 4.694811941647266e-05,
      "loss": 1.7563,
      "step": 575
    },
    {
      "epoch": 0.48058001035732784,
      "grad_norm": 1.046960711479187,
      "learning_rate": 4.6895943744400185e-05,
      "loss": 1.8407,
      "step": 580
    },
    {
      "epoch": 0.4847229414810979,
      "grad_norm": 1.025010108947754,
      "learning_rate": 4.6843355342728805e-05,
      "loss": 1.8229,
      "step": 585
    },
    {
      "epoch": 0.48886587260486797,
      "grad_norm": 0.981322705745697,
      "learning_rate": 4.679035520272842e-05,
      "loss": 1.7373,
      "step": 590
    },
    {
      "epoch": 0.49300880372863803,
      "grad_norm": 1.0959372520446777,
      "learning_rate": 4.673694432343002e-05,
      "loss": 1.7705,
      "step": 595
    },
    {
      "epoch": 0.4971517348524081,
      "grad_norm": 1.0281264781951904,
      "learning_rate": 4.668312371160688e-05,
      "loss": 1.8292,
      "step": 600
    },
    {
      "epoch": 0.5012946659761781,
      "grad_norm": 1.0642386674880981,
      "learning_rate": 4.662889438175555e-05,
      "loss": 1.7507,
      "step": 605
    },
    {
      "epoch": 0.5054375970999482,
      "grad_norm": 1.0140739679336548,
      "learning_rate": 4.657425735607676e-05,
      "loss": 1.7836,
      "step": 610
    },
    {
      "epoch": 0.5095805282237182,
      "grad_norm": 1.1006720066070557,
      "learning_rate": 4.651921366445613e-05,
      "loss": 1.7266,
      "step": 615
    },
    {
      "epoch": 0.5137234593474883,
      "grad_norm": 1.0564478635787964,
      "learning_rate": 4.646376434444477e-05,
      "loss": 1.8568,
      "step": 620
    },
    {
      "epoch": 0.5178663904712584,
      "grad_norm": 0.9862620234489441,
      "learning_rate": 4.640791044123973e-05,
      "loss": 1.8017,
      "step": 625
    },
    {
      "epoch": 0.5220093215950284,
      "grad_norm": 1.0272964239120483,
      "learning_rate": 4.6351653007664276e-05,
      "loss": 1.7442,
      "step": 630
    },
    {
      "epoch": 0.5261522527187985,
      "grad_norm": 0.9705718159675598,
      "learning_rate": 4.6294993104148045e-05,
      "loss": 1.7815,
      "step": 635
    },
    {
      "epoch": 0.5302951838425686,
      "grad_norm": 1.0880889892578125,
      "learning_rate": 4.62379317987071e-05,
      "loss": 1.7322,
      "step": 640
    },
    {
      "epoch": 0.5344381149663386,
      "grad_norm": 1.0593384504318237,
      "learning_rate": 4.618047016692374e-05,
      "loss": 1.6974,
      "step": 645
    },
    {
      "epoch": 0.5385810460901087,
      "grad_norm": 1.0683506727218628,
      "learning_rate": 4.612260929192627e-05,
      "loss": 1.6733,
      "step": 650
    },
    {
      "epoch": 0.5427239772138788,
      "grad_norm": 1.0647358894348145,
      "learning_rate": 4.606435026436854e-05,
      "loss": 1.8011,
      "step": 655
    },
    {
      "epoch": 0.5468669083376488,
      "grad_norm": 1.036792516708374,
      "learning_rate": 4.6005694182409456e-05,
      "loss": 1.7951,
      "step": 660
    },
    {
      "epoch": 0.5510098394614189,
      "grad_norm": 1.0680475234985352,
      "learning_rate": 4.594664215169219e-05,
      "loss": 1.7879,
      "step": 665
    },
    {
      "epoch": 0.555152770585189,
      "grad_norm": 1.1256674528121948,
      "learning_rate": 4.588719528532342e-05,
      "loss": 1.7433,
      "step": 670
    },
    {
      "epoch": 0.559295701708959,
      "grad_norm": 1.0891296863555908,
      "learning_rate": 4.582735470385229e-05,
      "loss": 1.7935,
      "step": 675
    },
    {
      "epoch": 0.5634386328327291,
      "grad_norm": 1.1049504280090332,
      "learning_rate": 4.5767121535249344e-05,
      "loss": 1.8395,
      "step": 680
    },
    {
      "epoch": 0.5675815639564992,
      "grad_norm": 0.9966709613800049,
      "learning_rate": 4.570649691488521e-05,
      "loss": 1.7992,
      "step": 685
    },
    {
      "epoch": 0.5717244950802693,
      "grad_norm": 1.0752801895141602,
      "learning_rate": 4.564548198550922e-05,
      "loss": 1.7575,
      "step": 690
    },
    {
      "epoch": 0.5758674262040394,
      "grad_norm": 1.0260655879974365,
      "learning_rate": 4.558407789722789e-05,
      "loss": 1.8119,
      "step": 695
    },
    {
      "epoch": 0.5800103573278095,
      "grad_norm": 1.101722002029419,
      "learning_rate": 4.55222858074832e-05,
      "loss": 1.7014,
      "step": 700
    },
    {
      "epoch": 0.5841532884515795,
      "grad_norm": 1.0106068849563599,
      "learning_rate": 4.5460106881030815e-05,
      "loss": 1.7807,
      "step": 705
    },
    {
      "epoch": 0.5882962195753496,
      "grad_norm": 1.1578617095947266,
      "learning_rate": 4.5397542289918116e-05,
      "loss": 1.7394,
      "step": 710
    },
    {
      "epoch": 0.5924391506991197,
      "grad_norm": 1.1157557964324951,
      "learning_rate": 4.533459321346209e-05,
      "loss": 1.7705,
      "step": 715
    },
    {
      "epoch": 0.5965820818228897,
      "grad_norm": 1.0634734630584717,
      "learning_rate": 4.5271260838227124e-05,
      "loss": 1.7808,
      "step": 720
    },
    {
      "epoch": 0.6007250129466598,
      "grad_norm": 1.2200394868850708,
      "learning_rate": 4.520754635800264e-05,
      "loss": 1.7851,
      "step": 725
    },
    {
      "epoch": 0.6048679440704299,
      "grad_norm": 1.095108985900879,
      "learning_rate": 4.514345097378057e-05,
      "loss": 1.7151,
      "step": 730
    },
    {
      "epoch": 0.6090108751941999,
      "grad_norm": 1.105036973953247,
      "learning_rate": 4.5078975893732725e-05,
      "loss": 1.7098,
      "step": 735
    },
    {
      "epoch": 0.61315380631797,
      "grad_norm": 1.1354011297225952,
      "learning_rate": 4.501412233318805e-05,
      "loss": 1.711,
      "step": 740
    },
    {
      "epoch": 0.6172967374417401,
      "grad_norm": 1.1678478717803955,
      "learning_rate": 4.494889151460967e-05,
      "loss": 1.7589,
      "step": 745
    },
    {
      "epoch": 0.6214396685655101,
      "grad_norm": 1.0858943462371826,
      "learning_rate": 4.488328466757189e-05,
      "loss": 1.7126,
      "step": 750
    },
    {
      "epoch": 0.6255825996892802,
      "grad_norm": 1.0415840148925781,
      "learning_rate": 4.481730302873698e-05,
      "loss": 1.8384,
      "step": 755
    },
    {
      "epoch": 0.6297255308130503,
      "grad_norm": 1.0674728155136108,
      "learning_rate": 4.4750947841831895e-05,
      "loss": 1.7563,
      "step": 760
    },
    {
      "epoch": 0.6338684619368203,
      "grad_norm": 1.1468995809555054,
      "learning_rate": 4.46842203576248e-05,
      "loss": 1.706,
      "step": 765
    },
    {
      "epoch": 0.6380113930605904,
      "grad_norm": 1.019692301750183,
      "learning_rate": 4.4617121833901545e-05,
      "loss": 1.7781,
      "step": 770
    },
    {
      "epoch": 0.6421543241843605,
      "grad_norm": 1.0939273834228516,
      "learning_rate": 4.454965353544189e-05,
      "loss": 1.7958,
      "step": 775
    },
    {
      "epoch": 0.6462972553081305,
      "grad_norm": 1.103552222251892,
      "learning_rate": 4.448181673399573e-05,
      "loss": 1.729,
      "step": 780
    },
    {
      "epoch": 0.6504401864319006,
      "grad_norm": 1.1372394561767578,
      "learning_rate": 4.441361270825906e-05,
      "loss": 1.7745,
      "step": 785
    },
    {
      "epoch": 0.6545831175556707,
      "grad_norm": 1.1918487548828125,
      "learning_rate": 4.4345042743849906e-05,
      "loss": 1.7641,
      "step": 790
    },
    {
      "epoch": 0.6587260486794407,
      "grad_norm": 1.1283992528915405,
      "learning_rate": 4.4276108133284114e-05,
      "loss": 1.7491,
      "step": 795
    },
    {
      "epoch": 0.6628689798032108,
      "grad_norm": 1.1781302690505981,
      "learning_rate": 4.4206810175950934e-05,
      "loss": 1.718,
      "step": 800
    },
    {
      "epoch": 0.6670119109269809,
      "grad_norm": 1.0509308576583862,
      "learning_rate": 4.4137150178088574e-05,
      "loss": 1.8248,
      "step": 805
    },
    {
      "epoch": 0.6711548420507509,
      "grad_norm": 1.1326382160186768,
      "learning_rate": 4.406712945275955e-05,
      "loss": 1.7283,
      "step": 810
    },
    {
      "epoch": 0.675297773174521,
      "grad_norm": 1.2352055311203003,
      "learning_rate": 4.399674931982594e-05,
      "loss": 1.8009,
      "step": 815
    },
    {
      "epoch": 0.6794407042982911,
      "grad_norm": 1.188057541847229,
      "learning_rate": 4.3926011105924515e-05,
      "loss": 1.6365,
      "step": 820
    },
    {
      "epoch": 0.6835836354220611,
      "grad_norm": 1.1620819568634033,
      "learning_rate": 4.385491614444171e-05,
      "loss": 1.762,
      "step": 825
    },
    {
      "epoch": 0.6877265665458312,
      "grad_norm": 1.1648989915847778,
      "learning_rate": 4.378346577548854e-05,
      "loss": 1.8709,
      "step": 830
    },
    {
      "epoch": 0.6918694976696013,
      "grad_norm": 1.0567896366119385,
      "learning_rate": 4.371166134587526e-05,
      "loss": 1.7982,
      "step": 835
    },
    {
      "epoch": 0.6960124287933713,
      "grad_norm": 1.1578034162521362,
      "learning_rate": 4.363950420908608e-05,
      "loss": 1.7395,
      "step": 840
    },
    {
      "epoch": 0.7001553599171414,
      "grad_norm": 1.129982590675354,
      "learning_rate": 4.356699572525354e-05,
      "loss": 1.6901,
      "step": 845
    },
    {
      "epoch": 0.7042982910409115,
      "grad_norm": 1.1160480976104736,
      "learning_rate": 4.349413726113296e-05,
      "loss": 1.6218,
      "step": 850
    },
    {
      "epoch": 0.7084412221646815,
      "grad_norm": 1.2563976049423218,
      "learning_rate": 4.342093019007664e-05,
      "loss": 1.816,
      "step": 855
    },
    {
      "epoch": 0.7125841532884516,
      "grad_norm": 1.1552455425262451,
      "learning_rate": 4.3347375892007974e-05,
      "loss": 1.7137,
      "step": 860
    },
    {
      "epoch": 0.7167270844122217,
      "grad_norm": 1.2065750360488892,
      "learning_rate": 4.327347575339545e-05,
      "loss": 1.7624,
      "step": 865
    },
    {
      "epoch": 0.7208700155359917,
      "grad_norm": 1.275793433189392,
      "learning_rate": 4.3199231167226506e-05,
      "loss": 1.7245,
      "step": 870
    },
    {
      "epoch": 0.7250129466597618,
      "grad_norm": 1.0843703746795654,
      "learning_rate": 4.312464353298127e-05,
      "loss": 1.7461,
      "step": 875
    },
    {
      "epoch": 0.7291558777835319,
      "grad_norm": 1.1525037288665771,
      "learning_rate": 4.304971425660619e-05,
      "loss": 1.7605,
      "step": 880
    },
    {
      "epoch": 0.7332988089073019,
      "grad_norm": 1.1188093423843384,
      "learning_rate": 4.297444475048755e-05,
      "loss": 1.8275,
      "step": 885
    },
    {
      "epoch": 0.737441740031072,
      "grad_norm": 1.2860270738601685,
      "learning_rate": 4.2898836433424775e-05,
      "loss": 1.647,
      "step": 890
    },
    {
      "epoch": 0.7415846711548421,
      "grad_norm": 1.1345618963241577,
      "learning_rate": 4.2822890730603795e-05,
      "loss": 1.7012,
      "step": 895
    },
    {
      "epoch": 0.7457276022786121,
      "grad_norm": 1.2110013961791992,
      "learning_rate": 4.274660907357009e-05,
      "loss": 1.6681,
      "step": 900
    },
    {
      "epoch": 0.7498705334023822,
      "grad_norm": 1.297300100326538,
      "learning_rate": 4.2669992900201735e-05,
      "loss": 1.7016,
      "step": 905
    },
    {
      "epoch": 0.7540134645261523,
      "grad_norm": 1.2929577827453613,
      "learning_rate": 4.2593043654682324e-05,
      "loss": 1.841,
      "step": 910
    },
    {
      "epoch": 0.7581563956499223,
      "grad_norm": 1.2006127834320068,
      "learning_rate": 4.251576278747372e-05,
      "loss": 1.7387,
      "step": 915
    },
    {
      "epoch": 0.7622993267736924,
      "grad_norm": 1.091860294342041,
      "learning_rate": 4.2438151755288704e-05,
      "loss": 1.7524,
      "step": 920
    },
    {
      "epoch": 0.7664422578974625,
      "grad_norm": 1.1326807737350464,
      "learning_rate": 4.236021202106355e-05,
      "loss": 1.7889,
      "step": 925
    },
    {
      "epoch": 0.7705851890212325,
      "grad_norm": 1.158422589302063,
      "learning_rate": 4.2281945053930414e-05,
      "loss": 1.7405,
      "step": 930
    },
    {
      "epoch": 0.7747281201450026,
      "grad_norm": 1.2753405570983887,
      "learning_rate": 4.2203352329189684e-05,
      "loss": 1.7325,
      "step": 935
    },
    {
      "epoch": 0.7788710512687727,
      "grad_norm": 1.1859126091003418,
      "learning_rate": 4.212443532828211e-05,
      "loss": 1.6476,
      "step": 940
    },
    {
      "epoch": 0.7830139823925427,
      "grad_norm": 1.1307568550109863,
      "learning_rate": 4.204519553876095e-05,
      "loss": 1.7539,
      "step": 945
    },
    {
      "epoch": 0.7871569135163128,
      "grad_norm": 1.1875184774398804,
      "learning_rate": 4.196563445426387e-05,
      "loss": 1.6942,
      "step": 950
    },
    {
      "epoch": 0.7912998446400829,
      "grad_norm": 1.2153640985488892,
      "learning_rate": 4.188575357448483e-05,
      "loss": 1.7799,
      "step": 955
    },
    {
      "epoch": 0.7954427757638529,
      "grad_norm": 1.2781250476837158,
      "learning_rate": 4.1805554405145805e-05,
      "loss": 1.7584,
      "step": 960
    },
    {
      "epoch": 0.799585706887623,
      "grad_norm": 1.3276715278625488,
      "learning_rate": 4.172503845796838e-05,
      "loss": 1.7384,
      "step": 965
    },
    {
      "epoch": 0.803728638011393,
      "grad_norm": 1.2462681531906128,
      "learning_rate": 4.1644207250645285e-05,
      "loss": 1.6268,
      "step": 970
    },
    {
      "epoch": 0.8078715691351631,
      "grad_norm": 1.3437224626541138,
      "learning_rate": 4.156306230681178e-05,
      "loss": 1.7711,
      "step": 975
    },
    {
      "epoch": 0.8120145002589332,
      "grad_norm": 1.159016728401184,
      "learning_rate": 4.148160515601691e-05,
      "loss": 1.6253,
      "step": 980
    },
    {
      "epoch": 0.8161574313827032,
      "grad_norm": 1.3188245296478271,
      "learning_rate": 4.1399837333694705e-05,
      "loss": 1.7,
      "step": 985
    },
    {
      "epoch": 0.8203003625064733,
      "grad_norm": 1.0977652072906494,
      "learning_rate": 4.131776038113524e-05,
      "loss": 1.6794,
      "step": 990
    },
    {
      "epoch": 0.8244432936302434,
      "grad_norm": 1.192744255065918,
      "learning_rate": 4.1235375845455555e-05,
      "loss": 1.7522,
      "step": 995
    },
    {
      "epoch": 0.8285862247540134,
      "grad_norm": 1.2207828760147095,
      "learning_rate": 4.1152685279570504e-05,
      "loss": 1.7017,
      "step": 1000
    },
    {
      "epoch": 0.8327291558777835,
      "grad_norm": 1.0897387266159058,
      "learning_rate": 4.1069690242163484e-05,
      "loss": 1.7454,
      "step": 1005
    },
    {
      "epoch": 0.8368720870015536,
      "grad_norm": 1.2813345193862915,
      "learning_rate": 4.098639229765707e-05,
      "loss": 1.6704,
      "step": 1010
    },
    {
      "epoch": 0.8410150181253236,
      "grad_norm": 1.2172951698303223,
      "learning_rate": 4.0902793016183505e-05,
      "loss": 1.8156,
      "step": 1015
    },
    {
      "epoch": 0.8451579492490937,
      "grad_norm": 1.3117797374725342,
      "learning_rate": 4.0818893973555096e-05,
      "loss": 1.7458,
      "step": 1020
    },
    {
      "epoch": 0.8493008803728638,
      "grad_norm": 1.2571377754211426,
      "learning_rate": 4.073469675123455e-05,
      "loss": 1.7703,
      "step": 1025
    },
    {
      "epoch": 0.8534438114966338,
      "grad_norm": 1.1640682220458984,
      "learning_rate": 4.065020293630513e-05,
      "loss": 1.695,
      "step": 1030
    },
    {
      "epoch": 0.8575867426204039,
      "grad_norm": 1.236506462097168,
      "learning_rate": 4.056541412144073e-05,
      "loss": 1.7329,
      "step": 1035
    },
    {
      "epoch": 0.861729673744174,
      "grad_norm": 1.210831880569458,
      "learning_rate": 4.04803319048759e-05,
      "loss": 1.6761,
      "step": 1040
    },
    {
      "epoch": 0.865872604867944,
      "grad_norm": 1.252960205078125,
      "learning_rate": 4.039495789037568e-05,
      "loss": 1.7851,
      "step": 1045
    },
    {
      "epoch": 0.8700155359917141,
      "grad_norm": 1.1295883655548096,
      "learning_rate": 4.030929368720539e-05,
      "loss": 1.8251,
      "step": 1050
    },
    {
      "epoch": 0.8741584671154842,
      "grad_norm": 1.2457700967788696,
      "learning_rate": 4.022334091010027e-05,
      "loss": 1.7984,
      "step": 1055
    },
    {
      "epoch": 0.8783013982392542,
      "grad_norm": 1.1537559032440186,
      "learning_rate": 4.013710117923508e-05,
      "loss": 1.771,
      "step": 1060
    },
    {
      "epoch": 0.8824443293630243,
      "grad_norm": 1.570165991783142,
      "learning_rate": 4.005057612019353e-05,
      "loss": 1.7266,
      "step": 1065
    },
    {
      "epoch": 0.8865872604867944,
      "grad_norm": 1.2282007932662964,
      "learning_rate": 3.996376736393764e-05,
      "loss": 1.7194,
      "step": 1070
    },
    {
      "epoch": 0.8907301916105644,
      "grad_norm": 1.2686131000518799,
      "learning_rate": 3.9876676546777045e-05,
      "loss": 1.6843,
      "step": 1075
    },
    {
      "epoch": 0.8948731227343345,
      "grad_norm": 1.267976999282837,
      "learning_rate": 3.978930531033807e-05,
      "loss": 1.5896,
      "step": 1080
    },
    {
      "epoch": 0.8990160538581046,
      "grad_norm": 1.2193140983581543,
      "learning_rate": 3.9701655301532836e-05,
      "loss": 1.7238,
      "step": 1085
    },
    {
      "epoch": 0.9031589849818746,
      "grad_norm": 1.2849549055099487,
      "learning_rate": 3.9613728172528244e-05,
      "loss": 1.6912,
      "step": 1090
    },
    {
      "epoch": 0.9073019161056447,
      "grad_norm": 1.3214191198349,
      "learning_rate": 3.952552558071475e-05,
      "loss": 1.7536,
      "step": 1095
    },
    {
      "epoch": 0.9114448472294148,
      "grad_norm": 1.11972177028656,
      "learning_rate": 3.943704918867521e-05,
      "loss": 1.7881,
      "step": 1100
    },
    {
      "epoch": 0.9155877783531848,
      "grad_norm": 1.2058112621307373,
      "learning_rate": 3.934830066415349e-05,
      "loss": 1.7669,
      "step": 1105
    },
    {
      "epoch": 0.9197307094769549,
      "grad_norm": 1.1645303964614868,
      "learning_rate": 3.925928168002302e-05,
      "loss": 1.7154,
      "step": 1110
    },
    {
      "epoch": 0.923873640600725,
      "grad_norm": 1.2252691984176636,
      "learning_rate": 3.916999391425532e-05,
      "loss": 1.7374,
      "step": 1115
    },
    {
      "epoch": 0.928016571724495,
      "grad_norm": 1.356494665145874,
      "learning_rate": 3.90804390498883e-05,
      "loss": 1.6987,
      "step": 1120
    },
    {
      "epoch": 0.9321595028482651,
      "grad_norm": 1.2140635251998901,
      "learning_rate": 3.8990618774994606e-05,
      "loss": 1.757,
      "step": 1125
    },
    {
      "epoch": 0.9363024339720352,
      "grad_norm": 1.4117001295089722,
      "learning_rate": 3.890053478264972e-05,
      "loss": 1.7456,
      "step": 1130
    },
    {
      "epoch": 0.9404453650958052,
      "grad_norm": 1.1910812854766846,
      "learning_rate": 3.881018877090013e-05,
      "loss": 1.7433,
      "step": 1135
    },
    {
      "epoch": 0.9445882962195753,
      "grad_norm": 1.291143774986267,
      "learning_rate": 3.871958244273127e-05,
      "loss": 1.6917,
      "step": 1140
    },
    {
      "epoch": 0.9487312273433454,
      "grad_norm": 1.1807619333267212,
      "learning_rate": 3.862871750603542e-05,
      "loss": 1.6407,
      "step": 1145
    },
    {
      "epoch": 0.9528741584671155,
      "grad_norm": 1.3264080286026,
      "learning_rate": 3.8537595673579534e-05,
      "loss": 1.6917,
      "step": 1150
    },
    {
      "epoch": 0.9570170895908856,
      "grad_norm": 1.4228436946868896,
      "learning_rate": 3.844621866297295e-05,
      "loss": 1.6528,
      "step": 1155
    },
    {
      "epoch": 0.9611600207146557,
      "grad_norm": 1.1992897987365723,
      "learning_rate": 3.835458819663501e-05,
      "loss": 1.809,
      "step": 1160
    },
    {
      "epoch": 0.9653029518384257,
      "grad_norm": 1.2063565254211426,
      "learning_rate": 3.8262706001762586e-05,
      "loss": 1.7004,
      "step": 1165
    },
    {
      "epoch": 0.9694458829621958,
      "grad_norm": 1.2039965391159058,
      "learning_rate": 3.817057381029752e-05,
      "loss": 1.6331,
      "step": 1170
    },
    {
      "epoch": 0.9735888140859659,
      "grad_norm": 1.2849379777908325,
      "learning_rate": 3.807819335889402e-05,
      "loss": 1.6646,
      "step": 1175
    },
    {
      "epoch": 0.9777317452097359,
      "grad_norm": 1.1539198160171509,
      "learning_rate": 3.7985566388885875e-05,
      "loss": 1.8006,
      "step": 1180
    },
    {
      "epoch": 0.981874676333506,
      "grad_norm": 1.2775242328643799,
      "learning_rate": 3.789269464625362e-05,
      "loss": 1.7992,
      "step": 1185
    },
    {
      "epoch": 0.9860176074572761,
      "grad_norm": 1.2502799034118652,
      "learning_rate": 3.779957988159172e-05,
      "loss": 1.6345,
      "step": 1190
    },
    {
      "epoch": 0.9901605385810461,
      "grad_norm": 1.4703786373138428,
      "learning_rate": 3.7706223850075454e-05,
      "loss": 1.7303,
      "step": 1195
    },
    {
      "epoch": 0.9943034697048162,
      "grad_norm": 1.3474452495574951,
      "learning_rate": 3.7612628311427875e-05,
      "loss": 1.6738,
      "step": 1200
    },
    {
      "epoch": 0.9984464008285863,
      "grad_norm": 1.2855440378189087,
      "learning_rate": 3.75187950298867e-05,
      "loss": 1.7736,
      "step": 1205
    },
    {
      "epoch": 1.0025893319523562,
      "grad_norm": 1.329331636428833,
      "learning_rate": 3.7424725774170945e-05,
      "loss": 1.6985,
      "step": 1210
    },
    {
      "epoch": 1.0067322630761264,
      "grad_norm": 1.1782236099243164,
      "learning_rate": 3.7330422317447685e-05,
      "loss": 1.7728,
      "step": 1215
    },
    {
      "epoch": 1.0108751941998964,
      "grad_norm": 1.2014517784118652,
      "learning_rate": 3.7235886437298576e-05,
      "loss": 1.7012,
      "step": 1220
    },
    {
      "epoch": 1.0150181253236665,
      "grad_norm": 1.3170599937438965,
      "learning_rate": 3.714111991568634e-05,
      "loss": 1.7529,
      "step": 1225
    },
    {
      "epoch": 1.0191610564474365,
      "grad_norm": 1.2770780324935913,
      "learning_rate": 3.704612453892123e-05,
      "loss": 1.7387,
      "step": 1230
    },
    {
      "epoch": 1.0233039875712067,
      "grad_norm": 1.2918888330459595,
      "learning_rate": 3.695090209762731e-05,
      "loss": 1.7119,
      "step": 1235
    },
    {
      "epoch": 1.0274469186949766,
      "grad_norm": 1.3758825063705444,
      "learning_rate": 3.68554543867087e-05,
      "loss": 1.6657,
      "step": 1240
    },
    {
      "epoch": 1.0315898498187468,
      "grad_norm": 1.2998440265655518,
      "learning_rate": 3.675978320531579e-05,
      "loss": 1.6265,
      "step": 1245
    },
    {
      "epoch": 1.0357327809425168,
      "grad_norm": 1.3095053434371948,
      "learning_rate": 3.6663890356811286e-05,
      "loss": 1.6224,
      "step": 1250
    },
    {
      "epoch": 1.039875712066287,
      "grad_norm": 1.316596269607544,
      "learning_rate": 3.656777764873622e-05,
      "loss": 1.7796,
      "step": 1255
    },
    {
      "epoch": 1.0440186431900569,
      "grad_norm": 1.294472575187683,
      "learning_rate": 3.6471446892775895e-05,
      "loss": 1.6427,
      "step": 1260
    },
    {
      "epoch": 1.048161574313827,
      "grad_norm": 1.2268025875091553,
      "learning_rate": 3.637489990472573e-05,
      "loss": 1.6522,
      "step": 1265
    },
    {
      "epoch": 1.052304505437597,
      "grad_norm": 1.240159511566162,
      "learning_rate": 3.627813850445702e-05,
      "loss": 1.7156,
      "step": 1270
    },
    {
      "epoch": 1.0564474365613672,
      "grad_norm": 1.2868086099624634,
      "learning_rate": 3.618116451588266e-05,
      "loss": 1.6792,
      "step": 1275
    },
    {
      "epoch": 1.0605903676851371,
      "grad_norm": 1.2162740230560303,
      "learning_rate": 3.6083979766922724e-05,
      "loss": 1.6978,
      "step": 1280
    },
    {
      "epoch": 1.0647332988089073,
      "grad_norm": 1.4078983068466187,
      "learning_rate": 3.5986586089470064e-05,
      "loss": 1.7169,
      "step": 1285
    },
    {
      "epoch": 1.0688762299326773,
      "grad_norm": 1.369338035583496,
      "learning_rate": 3.588898531935573e-05,
      "loss": 1.5987,
      "step": 1290
    },
    {
      "epoch": 1.0730191610564475,
      "grad_norm": 1.3661071062088013,
      "learning_rate": 3.5791179296314364e-05,
      "loss": 1.6967,
      "step": 1295
    },
    {
      "epoch": 1.0771620921802174,
      "grad_norm": 1.2916308641433716,
      "learning_rate": 3.5693169863949585e-05,
      "loss": 1.6882,
      "step": 1300
    },
    {
      "epoch": 1.0813050233039876,
      "grad_norm": 1.2639518976211548,
      "learning_rate": 3.5594958869699155e-05,
      "loss": 1.7198,
      "step": 1305
    },
    {
      "epoch": 1.0854479544277575,
      "grad_norm": 1.383553385734558,
      "learning_rate": 3.5496548164800234e-05,
      "loss": 1.6419,
      "step": 1310
    },
    {
      "epoch": 1.0895908855515277,
      "grad_norm": 1.2819234132766724,
      "learning_rate": 3.539793960425443e-05,
      "loss": 1.6807,
      "step": 1315
    },
    {
      "epoch": 1.0937338166752977,
      "grad_norm": 1.1928539276123047,
      "learning_rate": 3.5299135046792815e-05,
      "loss": 1.7799,
      "step": 1320
    },
    {
      "epoch": 1.0978767477990679,
      "grad_norm": 1.2940860986709595,
      "learning_rate": 3.5200136354840974e-05,
      "loss": 1.8522,
      "step": 1325
    },
    {
      "epoch": 1.1020196789228378,
      "grad_norm": 1.361895203590393,
      "learning_rate": 3.510094539448382e-05,
      "loss": 1.6605,
      "step": 1330
    },
    {
      "epoch": 1.106162610046608,
      "grad_norm": 1.2896027565002441,
      "learning_rate": 3.5001564035430455e-05,
      "loss": 1.6635,
      "step": 1335
    },
    {
      "epoch": 1.110305541170378,
      "grad_norm": 1.3783482313156128,
      "learning_rate": 3.490199415097892e-05,
      "loss": 1.7006,
      "step": 1340
    },
    {
      "epoch": 1.1144484722941481,
      "grad_norm": 1.3359092473983765,
      "learning_rate": 3.4802237617980884e-05,
      "loss": 1.745,
      "step": 1345
    },
    {
      "epoch": 1.118591403417918,
      "grad_norm": 1.34321129322052,
      "learning_rate": 3.4702296316806244e-05,
      "loss": 1.7022,
      "step": 1350
    },
    {
      "epoch": 1.1227343345416883,
      "grad_norm": 1.3096846342086792,
      "learning_rate": 3.4602172131307734e-05,
      "loss": 1.6198,
      "step": 1355
    },
    {
      "epoch": 1.1268772656654584,
      "grad_norm": 1.311469316482544,
      "learning_rate": 3.450186694878536e-05,
      "loss": 1.7563,
      "step": 1360
    },
    {
      "epoch": 1.1310201967892284,
      "grad_norm": 1.341980218887329,
      "learning_rate": 3.4401382659950864e-05,
      "loss": 1.718,
      "step": 1365
    },
    {
      "epoch": 1.1351631279129983,
      "grad_norm": 1.3548580408096313,
      "learning_rate": 3.4300721158892043e-05,
      "loss": 1.7112,
      "step": 1370
    },
    {
      "epoch": 1.1393060590367685,
      "grad_norm": 1.2631733417510986,
      "learning_rate": 3.4199884343037116e-05,
      "loss": 1.6686,
      "step": 1375
    },
    {
      "epoch": 1.1434489901605387,
      "grad_norm": 1.3094624280929565,
      "learning_rate": 3.409887411311886e-05,
      "loss": 1.713,
      "step": 1380
    },
    {
      "epoch": 1.1475919212843086,
      "grad_norm": 1.5153518915176392,
      "learning_rate": 3.3997692373138884e-05,
      "loss": 1.618,
      "step": 1385
    },
    {
      "epoch": 1.1517348524080786,
      "grad_norm": 1.3567681312561035,
      "learning_rate": 3.3896341030331655e-05,
      "loss": 1.6609,
      "step": 1390
    },
    {
      "epoch": 1.1558777835318488,
      "grad_norm": 1.230713129043579,
      "learning_rate": 3.379482199512861e-05,
      "loss": 1.7109,
      "step": 1395
    },
    {
      "epoch": 1.160020714655619,
      "grad_norm": 1.2525748014450073,
      "learning_rate": 3.369313718112211e-05,
      "loss": 1.6506,
      "step": 1400
    },
    {
      "epoch": 1.164163645779389,
      "grad_norm": 1.4338536262512207,
      "learning_rate": 3.359128850502938e-05,
      "loss": 1.6942,
      "step": 1405
    },
    {
      "epoch": 1.1683065769031589,
      "grad_norm": 1.3373507261276245,
      "learning_rate": 3.348927788665637e-05,
      "loss": 1.7934,
      "step": 1410
    },
    {
      "epoch": 1.172449508026929,
      "grad_norm": 1.3026612997055054,
      "learning_rate": 3.338710724886159e-05,
      "loss": 1.7061,
      "step": 1415
    },
    {
      "epoch": 1.1765924391506992,
      "grad_norm": 1.3221571445465088,
      "learning_rate": 3.328477851751984e-05,
      "loss": 1.7688,
      "step": 1420
    },
    {
      "epoch": 1.1807353702744692,
      "grad_norm": 1.3228507041931152,
      "learning_rate": 3.318229362148592e-05,
      "loss": 1.7277,
      "step": 1425
    },
    {
      "epoch": 1.1848783013982391,
      "grad_norm": 1.3517146110534668,
      "learning_rate": 3.307965449255828e-05,
      "loss": 1.6902,
      "step": 1430
    },
    {
      "epoch": 1.1890212325220093,
      "grad_norm": 1.224722981452942,
      "learning_rate": 3.2976863065442584e-05,
      "loss": 1.7773,
      "step": 1435
    },
    {
      "epoch": 1.1931641636457795,
      "grad_norm": 1.3550224304199219,
      "learning_rate": 3.287392127771526e-05,
      "loss": 1.6376,
      "step": 1440
    },
    {
      "epoch": 1.1973070947695494,
      "grad_norm": 1.3210028409957886,
      "learning_rate": 3.2770831069786976e-05,
      "loss": 1.7077,
      "step": 1445
    },
    {
      "epoch": 1.2014500258933196,
      "grad_norm": 1.2369807958602905,
      "learning_rate": 3.266759438486606e-05,
      "loss": 1.7218,
      "step": 1450
    },
    {
      "epoch": 1.2055929570170896,
      "grad_norm": 1.2823275327682495,
      "learning_rate": 3.256421316892187e-05,
      "loss": 1.7345,
      "step": 1455
    },
    {
      "epoch": 1.2097358881408597,
      "grad_norm": 1.316133737564087,
      "learning_rate": 3.2460689370648095e-05,
      "loss": 1.6833,
      "step": 1460
    },
    {
      "epoch": 1.2138788192646297,
      "grad_norm": 1.327427625656128,
      "learning_rate": 3.235702494142608e-05,
      "loss": 1.7298,
      "step": 1465
    },
    {
      "epoch": 1.2180217503883999,
      "grad_norm": 1.4731863737106323,
      "learning_rate": 3.2253221835287984e-05,
      "loss": 1.6964,
      "step": 1470
    },
    {
      "epoch": 1.2221646815121698,
      "grad_norm": 1.2221453189849854,
      "learning_rate": 3.2149282008879966e-05,
      "loss": 1.7581,
      "step": 1475
    },
    {
      "epoch": 1.22630761263594,
      "grad_norm": 1.4211440086364746,
      "learning_rate": 3.204520742142532e-05,
      "loss": 1.7185,
      "step": 1480
    },
    {
      "epoch": 1.23045054375971,
      "grad_norm": 1.3190487623214722,
      "learning_rate": 3.1941000034687515e-05,
      "loss": 1.8047,
      "step": 1485
    },
    {
      "epoch": 1.2345934748834801,
      "grad_norm": 1.3464876413345337,
      "learning_rate": 3.183666181293325e-05,
      "loss": 1.7077,
      "step": 1490
    },
    {
      "epoch": 1.23873640600725,
      "grad_norm": 1.3570501804351807,
      "learning_rate": 3.1732194722895394e-05,
      "loss": 1.5651,
      "step": 1495
    },
    {
      "epoch": 1.2428793371310203,
      "grad_norm": 1.4711155891418457,
      "learning_rate": 3.162760073373594e-05,
      "loss": 1.6612,
      "step": 1500
    },
    {
      "epoch": 1.2470222682547902,
      "grad_norm": 1.2608466148376465,
      "learning_rate": 3.152288181700887e-05,
      "loss": 1.6882,
      "step": 1505
    },
    {
      "epoch": 1.2511651993785604,
      "grad_norm": 1.402365803718567,
      "learning_rate": 3.141803994662301e-05,
      "loss": 1.6911,
      "step": 1510
    },
    {
      "epoch": 1.2553081305023304,
      "grad_norm": 1.369901418685913,
      "learning_rate": 3.1313077098804815e-05,
      "loss": 1.7251,
      "step": 1515
    },
    {
      "epoch": 1.2594510616261005,
      "grad_norm": 1.2668217420578003,
      "learning_rate": 3.12079952520611e-05,
      "loss": 1.7408,
      "step": 1520
    },
    {
      "epoch": 1.2635939927498705,
      "grad_norm": 1.3345353603363037,
      "learning_rate": 3.1102796387141754e-05,
      "loss": 1.6261,
      "step": 1525
    },
    {
      "epoch": 1.2677369238736407,
      "grad_norm": 1.2580366134643555,
      "learning_rate": 3.099748248700245e-05,
      "loss": 1.6183,
      "step": 1530
    },
    {
      "epoch": 1.2718798549974106,
      "grad_norm": 1.2078744173049927,
      "learning_rate": 3.089205553676719e-05,
      "loss": 1.6418,
      "step": 1535
    },
    {
      "epoch": 1.2760227861211808,
      "grad_norm": 1.2932053804397583,
      "learning_rate": 3.0786517523690934e-05,
      "loss": 1.8163,
      "step": 1540
    },
    {
      "epoch": 1.2801657172449508,
      "grad_norm": 1.280853033065796,
      "learning_rate": 3.0680870437122145e-05,
      "loss": 1.7337,
      "step": 1545
    },
    {
      "epoch": 1.284308648368721,
      "grad_norm": 1.2563878297805786,
      "learning_rate": 3.0575116268465265e-05,
      "loss": 1.6889,
      "step": 1550
    },
    {
      "epoch": 1.288451579492491,
      "grad_norm": 1.3088014125823975,
      "learning_rate": 3.046925701114318e-05,
      "loss": 1.6607,
      "step": 1555
    },
    {
      "epoch": 1.292594510616261,
      "grad_norm": 1.3936069011688232,
      "learning_rate": 3.0363294660559682e-05,
      "loss": 1.628,
      "step": 1560
    },
    {
      "epoch": 1.296737441740031,
      "grad_norm": 1.3524973392486572,
      "learning_rate": 3.0257231214061794e-05,
      "loss": 1.6847,
      "step": 1565
    },
    {
      "epoch": 1.3008803728638012,
      "grad_norm": 1.3065828084945679,
      "learning_rate": 3.0151068670902183e-05,
      "loss": 1.618,
      "step": 1570
    },
    {
      "epoch": 1.3050233039875712,
      "grad_norm": 1.293328046798706,
      "learning_rate": 3.0044809032201447e-05,
      "loss": 1.7261,
      "step": 1575
    },
    {
      "epoch": 1.3091662351113413,
      "grad_norm": 1.4054830074310303,
      "learning_rate": 2.9938454300910378e-05,
      "loss": 1.7306,
      "step": 1580
    },
    {
      "epoch": 1.3133091662351113,
      "grad_norm": 1.3852732181549072,
      "learning_rate": 2.9832006481772233e-05,
      "loss": 1.6513,
      "step": 1585
    },
    {
      "epoch": 1.3174520973588815,
      "grad_norm": 1.3759453296661377,
      "learning_rate": 2.9725467581284944e-05,
      "loss": 1.702,
      "step": 1590
    },
    {
      "epoch": 1.3215950284826514,
      "grad_norm": 1.3048088550567627,
      "learning_rate": 2.9618839607663275e-05,
      "loss": 1.6714,
      "step": 1595
    },
    {
      "epoch": 1.3257379596064216,
      "grad_norm": 1.2825406789779663,
      "learning_rate": 2.9512124570800986e-05,
      "loss": 1.6358,
      "step": 1600
    },
    {
      "epoch": 1.3298808907301916,
      "grad_norm": 1.3857752084732056,
      "learning_rate": 2.940532448223296e-05,
      "loss": 1.624,
      "step": 1605
    },
    {
      "epoch": 1.3340238218539617,
      "grad_norm": 1.287382960319519,
      "learning_rate": 2.929844135509725e-05,
      "loss": 1.7086,
      "step": 1610
    },
    {
      "epoch": 1.3381667529777317,
      "grad_norm": 1.3335013389587402,
      "learning_rate": 2.9191477204097156e-05,
      "loss": 1.729,
      "step": 1615
    },
    {
      "epoch": 1.3423096841015019,
      "grad_norm": 1.50087571144104,
      "learning_rate": 2.9084434045463255e-05,
      "loss": 1.6616,
      "step": 1620
    },
    {
      "epoch": 1.3464526152252718,
      "grad_norm": 1.320407748222351,
      "learning_rate": 2.897731389691538e-05,
      "loss": 1.5987,
      "step": 1625
    },
    {
      "epoch": 1.350595546349042,
      "grad_norm": 1.3943419456481934,
      "learning_rate": 2.8870118777624583e-05,
      "loss": 1.6178,
      "step": 1630
    },
    {
      "epoch": 1.354738477472812,
      "grad_norm": 1.3598078489303589,
      "learning_rate": 2.8762850708175098e-05,
      "loss": 1.678,
      "step": 1635
    },
    {
      "epoch": 1.3588814085965821,
      "grad_norm": 1.3767399787902832,
      "learning_rate": 2.865551171052624e-05,
      "loss": 1.6268,
      "step": 1640
    },
    {
      "epoch": 1.363024339720352,
      "grad_norm": 1.5609691143035889,
      "learning_rate": 2.854810380797427e-05,
      "loss": 1.7446,
      "step": 1645
    },
    {
      "epoch": 1.3671672708441223,
      "grad_norm": 1.4516915082931519,
      "learning_rate": 2.8440629025114306e-05,
      "loss": 1.641,
      "step": 1650
    },
    {
      "epoch": 1.3713102019678922,
      "grad_norm": 1.3061145544052124,
      "learning_rate": 2.833308938780211e-05,
      "loss": 1.6905,
      "step": 1655
    },
    {
      "epoch": 1.3754531330916624,
      "grad_norm": 1.3689554929733276,
      "learning_rate": 2.8225486923115935e-05,
      "loss": 1.6551,
      "step": 1660
    },
    {
      "epoch": 1.3795960642154323,
      "grad_norm": 1.3454234600067139,
      "learning_rate": 2.811782365931832e-05,
      "loss": 1.8066,
      "step": 1665
    },
    {
      "epoch": 1.3837389953392025,
      "grad_norm": 1.4044209718704224,
      "learning_rate": 2.8010101625817815e-05,
      "loss": 1.7489,
      "step": 1670
    },
    {
      "epoch": 1.3878819264629725,
      "grad_norm": 1.3559893369674683,
      "learning_rate": 2.7902322853130757e-05,
      "loss": 1.6984,
      "step": 1675
    },
    {
      "epoch": 1.3920248575867427,
      "grad_norm": 1.4070065021514893,
      "learning_rate": 2.779448937284302e-05,
      "loss": 1.6392,
      "step": 1680
    },
    {
      "epoch": 1.3961677887105126,
      "grad_norm": 1.4083309173583984,
      "learning_rate": 2.768660321757166e-05,
      "loss": 1.7071,
      "step": 1685
    },
    {
      "epoch": 1.4003107198342828,
      "grad_norm": 1.383867621421814,
      "learning_rate": 2.7578666420926658e-05,
      "loss": 1.6376,
      "step": 1690
    },
    {
      "epoch": 1.4044536509580527,
      "grad_norm": 1.2637907266616821,
      "learning_rate": 2.7470681017472554e-05,
      "loss": 1.7565,
      "step": 1695
    },
    {
      "epoch": 1.408596582081823,
      "grad_norm": 1.374517560005188,
      "learning_rate": 2.7362649042690114e-05,
      "loss": 1.7832,
      "step": 1700
    },
    {
      "epoch": 1.4127395132055929,
      "grad_norm": 1.4987614154815674,
      "learning_rate": 2.7254572532937957e-05,
      "loss": 1.6548,
      "step": 1705
    },
    {
      "epoch": 1.416882444329363,
      "grad_norm": 1.4538588523864746,
      "learning_rate": 2.714645352541415e-05,
      "loss": 1.6151,
      "step": 1710
    },
    {
      "epoch": 1.421025375453133,
      "grad_norm": 1.3836619853973389,
      "learning_rate": 2.7038294058117846e-05,
      "loss": 1.6243,
      "step": 1715
    },
    {
      "epoch": 1.4251683065769032,
      "grad_norm": 1.4926493167877197,
      "learning_rate": 2.6930096169810825e-05,
      "loss": 1.68,
      "step": 1720
    },
    {
      "epoch": 1.4293112377006731,
      "grad_norm": 1.4516563415527344,
      "learning_rate": 2.6821861899979118e-05,
      "loss": 1.7081,
      "step": 1725
    },
    {
      "epoch": 1.4334541688244433,
      "grad_norm": 1.3520265817642212,
      "learning_rate": 2.671359328879451e-05,
      "loss": 1.7115,
      "step": 1730
    },
    {
      "epoch": 1.4375970999482135,
      "grad_norm": 1.3310267925262451,
      "learning_rate": 2.660529237707611e-05,
      "loss": 1.6334,
      "step": 1735
    },
    {
      "epoch": 1.4417400310719835,
      "grad_norm": 1.4125117063522339,
      "learning_rate": 2.649696120625188e-05,
      "loss": 1.6734,
      "step": 1740
    },
    {
      "epoch": 1.4458829621957534,
      "grad_norm": 1.439457893371582,
      "learning_rate": 2.6388601818320163e-05,
      "loss": 1.712,
      "step": 1745
    },
    {
      "epoch": 1.4500258933195236,
      "grad_norm": 1.3236662149429321,
      "learning_rate": 2.628021625581117e-05,
      "loss": 1.678,
      "step": 1750
    },
    {
      "epoch": 1.4541688244432938,
      "grad_norm": 1.36537766456604,
      "learning_rate": 2.6171806561748502e-05,
      "loss": 1.7226,
      "step": 1755
    },
    {
      "epoch": 1.4583117555670637,
      "grad_norm": 1.343255639076233,
      "learning_rate": 2.6063374779610626e-05,
      "loss": 1.6526,
      "step": 1760
    },
    {
      "epoch": 1.4624546866908337,
      "grad_norm": 1.3841335773468018,
      "learning_rate": 2.595492295329236e-05,
      "loss": 1.6965,
      "step": 1765
    },
    {
      "epoch": 1.4665976178146038,
      "grad_norm": 1.3960946798324585,
      "learning_rate": 2.584645312706634e-05,
      "loss": 1.6541,
      "step": 1770
    },
    {
      "epoch": 1.470740548938374,
      "grad_norm": 1.6239416599273682,
      "learning_rate": 2.5737967345544502e-05,
      "loss": 1.6999,
      "step": 1775
    },
    {
      "epoch": 1.474883480062144,
      "grad_norm": 1.27378249168396,
      "learning_rate": 2.5629467653639532e-05,
      "loss": 1.6288,
      "step": 1780
    },
    {
      "epoch": 1.479026411185914,
      "grad_norm": 1.4293031692504883,
      "learning_rate": 2.552095609652632e-05,
      "loss": 1.6344,
      "step": 1785
    },
    {
      "epoch": 1.4831693423096841,
      "grad_norm": 1.3467810153961182,
      "learning_rate": 2.5412434719603407e-05,
      "loss": 1.7231,
      "step": 1790
    },
    {
      "epoch": 1.4873122734334543,
      "grad_norm": 1.3248693943023682,
      "learning_rate": 2.530390556845444e-05,
      "loss": 1.6853,
      "step": 1795
    },
    {
      "epoch": 1.4914552045572242,
      "grad_norm": 1.5432088375091553,
      "learning_rate": 2.51953706888096e-05,
      "loss": 1.6094,
      "step": 1800
    },
    {
      "epoch": 1.4955981356809942,
      "grad_norm": 1.3911875486373901,
      "learning_rate": 2.508683212650705e-05,
      "loss": 1.7059,
      "step": 1805
    },
    {
      "epoch": 1.4997410668047644,
      "grad_norm": 1.3618335723876953,
      "learning_rate": 2.4978291927454368e-05,
      "loss": 1.942,
      "step": 1810
    },
    {
      "epoch": 1.5038839979285346,
      "grad_norm": 1.3595517873764038,
      "learning_rate": 2.486975213758999e-05,
      "loss": 1.8394,
      "step": 1815
    },
    {
      "epoch": 1.5080269290523045,
      "grad_norm": 1.3139194250106812,
      "learning_rate": 2.4761214802844635e-05,
      "loss": 1.7196,
      "step": 1820
    },
    {
      "epoch": 1.5121698601760745,
      "grad_norm": 1.4064507484436035,
      "learning_rate": 2.4652681969102755e-05,
      "loss": 1.5871,
      "step": 1825
    },
    {
      "epoch": 1.5163127912998446,
      "grad_norm": 1.4719702005386353,
      "learning_rate": 2.454415568216392e-05,
      "loss": 1.6958,
      "step": 1830
    },
    {
      "epoch": 1.5204557224236148,
      "grad_norm": 1.4541914463043213,
      "learning_rate": 2.443563798770436e-05,
      "loss": 1.6534,
      "step": 1835
    },
    {
      "epoch": 1.5245986535473848,
      "grad_norm": 1.4805889129638672,
      "learning_rate": 2.4327130931238273e-05,
      "loss": 1.6354,
      "step": 1840
    },
    {
      "epoch": 1.5287415846711547,
      "grad_norm": 1.4338195323944092,
      "learning_rate": 2.4218636558079398e-05,
      "loss": 1.6406,
      "step": 1845
    },
    {
      "epoch": 1.532884515794925,
      "grad_norm": 1.433242917060852,
      "learning_rate": 2.4110156913302343e-05,
      "loss": 1.7508,
      "step": 1850
    },
    {
      "epoch": 1.537027446918695,
      "grad_norm": 1.4096941947937012,
      "learning_rate": 2.4001694041704122e-05,
      "loss": 1.7593,
      "step": 1855
    },
    {
      "epoch": 1.541170378042465,
      "grad_norm": 1.434018611907959,
      "learning_rate": 2.3893249987765595e-05,
      "loss": 1.6649,
      "step": 1860
    },
    {
      "epoch": 1.545313309166235,
      "grad_norm": 1.3782891035079956,
      "learning_rate": 2.3784826795612875e-05,
      "loss": 1.6538,
      "step": 1865
    },
    {
      "epoch": 1.5494562402900052,
      "grad_norm": 1.4486429691314697,
      "learning_rate": 2.3676426508978873e-05,
      "loss": 1.6864,
      "step": 1870
    },
    {
      "epoch": 1.5535991714137753,
      "grad_norm": 1.6130430698394775,
      "learning_rate": 2.3568051171164723e-05,
      "loss": 1.7062,
      "step": 1875
    },
    {
      "epoch": 1.5577421025375453,
      "grad_norm": 1.217040777206421,
      "learning_rate": 2.3459702825001306e-05,
      "loss": 1.7507,
      "step": 1880
    },
    {
      "epoch": 1.5618850336613153,
      "grad_norm": 1.61445152759552,
      "learning_rate": 2.3351383512810685e-05,
      "loss": 1.6397,
      "step": 1885
    },
    {
      "epoch": 1.5660279647850854,
      "grad_norm": 1.4629881381988525,
      "learning_rate": 2.3243095276367685e-05,
      "loss": 1.6435,
      "step": 1890
    },
    {
      "epoch": 1.5701708959088556,
      "grad_norm": 1.368586778640747,
      "learning_rate": 2.3134840156861327e-05,
      "loss": 1.7338,
      "step": 1895
    },
    {
      "epoch": 1.5743138270326256,
      "grad_norm": 1.2673254013061523,
      "learning_rate": 2.3026620194856415e-05,
      "loss": 1.7511,
      "step": 1900
    },
    {
      "epoch": 1.5784567581563955,
      "grad_norm": 1.3362444639205933,
      "learning_rate": 2.2918437430255056e-05,
      "loss": 1.6821,
      "step": 1905
    },
    {
      "epoch": 1.5825996892801657,
      "grad_norm": 1.2994166612625122,
      "learning_rate": 2.281029390225818e-05,
      "loss": 1.7014,
      "step": 1910
    },
    {
      "epoch": 1.5867426204039359,
      "grad_norm": 1.3709162473678589,
      "learning_rate": 2.270219164932714e-05,
      "loss": 1.759,
      "step": 1915
    },
    {
      "epoch": 1.5908855515277058,
      "grad_norm": 1.5352706909179688,
      "learning_rate": 2.2594132709145245e-05,
      "loss": 1.6507,
      "step": 1920
    },
    {
      "epoch": 1.5950284826514758,
      "grad_norm": 1.4126780033111572,
      "learning_rate": 2.248611911857941e-05,
      "loss": 1.6437,
      "step": 1925
    },
    {
      "epoch": 1.599171413775246,
      "grad_norm": 1.3970434665679932,
      "learning_rate": 2.2378152913641705e-05,
      "loss": 1.674,
      "step": 1930
    },
    {
      "epoch": 1.6033143448990161,
      "grad_norm": 1.3854368925094604,
      "learning_rate": 2.227023612945102e-05,
      "loss": 1.6489,
      "step": 1935
    },
    {
      "epoch": 1.607457276022786,
      "grad_norm": 1.4206554889678955,
      "learning_rate": 2.216237080019465e-05,
      "loss": 1.6625,
      "step": 1940
    },
    {
      "epoch": 1.611600207146556,
      "grad_norm": 1.3551628589630127,
      "learning_rate": 2.2054558959090018e-05,
      "loss": 1.7448,
      "step": 1945
    },
    {
      "epoch": 1.6157431382703262,
      "grad_norm": 1.6237330436706543,
      "learning_rate": 2.1946802638346323e-05,
      "loss": 1.7287,
      "step": 1950
    },
    {
      "epoch": 1.6198860693940964,
      "grad_norm": 1.5089672803878784,
      "learning_rate": 2.1839103869126183e-05,
      "loss": 1.5997,
      "step": 1955
    },
    {
      "epoch": 1.6240290005178664,
      "grad_norm": 1.3349740505218506,
      "learning_rate": 2.173146468150744e-05,
      "loss": 1.6943,
      "step": 1960
    },
    {
      "epoch": 1.6281719316416363,
      "grad_norm": 1.385046362876892,
      "learning_rate": 2.1623887104444816e-05,
      "loss": 1.6961,
      "step": 1965
    },
    {
      "epoch": 1.6323148627654065,
      "grad_norm": 1.4786276817321777,
      "learning_rate": 2.1516373165731736e-05,
      "loss": 1.7331,
      "step": 1970
    },
    {
      "epoch": 1.6364577938891767,
      "grad_norm": 1.4804706573486328,
      "learning_rate": 2.1408924891962014e-05,
      "loss": 1.6679,
      "step": 1975
    },
    {
      "epoch": 1.6406007250129466,
      "grad_norm": 1.4840457439422607,
      "learning_rate": 2.1301544308491754e-05,
      "loss": 1.6326,
      "step": 1980
    },
    {
      "epoch": 1.6447436561367166,
      "grad_norm": 1.4239140748977661,
      "learning_rate": 2.119423343940111e-05,
      "loss": 1.7688,
      "step": 1985
    },
    {
      "epoch": 1.6488865872604868,
      "grad_norm": 1.4560587406158447,
      "learning_rate": 2.108699430745613e-05,
      "loss": 1.7557,
      "step": 1990
    },
    {
      "epoch": 1.653029518384257,
      "grad_norm": 1.312207818031311,
      "learning_rate": 2.097982893407068e-05,
      "loss": 1.7119,
      "step": 1995
    },
    {
      "epoch": 1.657172449508027,
      "grad_norm": 1.421198844909668,
      "learning_rate": 2.087273933926828e-05,
      "loss": 1.6473,
      "step": 2000
    },
    {
      "epoch": 1.6613153806317968,
      "grad_norm": 1.4247469902038574,
      "learning_rate": 2.076572754164408e-05,
      "loss": 1.7138,
      "step": 2005
    },
    {
      "epoch": 1.665458311755567,
      "grad_norm": 1.4015504121780396,
      "learning_rate": 2.0658795558326743e-05,
      "loss": 1.7403,
      "step": 2010
    },
    {
      "epoch": 1.6696012428793372,
      "grad_norm": 1.2809052467346191,
      "learning_rate": 2.0551945404940515e-05,
      "loss": 1.6717,
      "step": 2015
    },
    {
      "epoch": 1.6737441740031072,
      "grad_norm": 1.3892935514450073,
      "learning_rate": 2.044517909556714e-05,
      "loss": 1.768,
      "step": 2020
    },
    {
      "epoch": 1.677887105126877,
      "grad_norm": 1.4041610956192017,
      "learning_rate": 2.0338498642707977e-05,
      "loss": 1.8498,
      "step": 2025
    },
    {
      "epoch": 1.6820300362506473,
      "grad_norm": 1.3415899276733398,
      "learning_rate": 2.0231906057245975e-05,
      "loss": 1.6846,
      "step": 2030
    },
    {
      "epoch": 1.6861729673744175,
      "grad_norm": 1.4206855297088623,
      "learning_rate": 2.012540334840786e-05,
      "loss": 1.7245,
      "step": 2035
    },
    {
      "epoch": 1.6903158984981874,
      "grad_norm": 1.2831535339355469,
      "learning_rate": 2.0018992523726215e-05,
      "loss": 1.8021,
      "step": 2040
    },
    {
      "epoch": 1.6944588296219574,
      "grad_norm": 1.5180027484893799,
      "learning_rate": 1.9912675589001617e-05,
      "loss": 1.6908,
      "step": 2045
    },
    {
      "epoch": 1.6986017607457276,
      "grad_norm": 1.6339993476867676,
      "learning_rate": 1.9806454548264882e-05,
      "loss": 1.7302,
      "step": 2050
    },
    {
      "epoch": 1.7027446918694977,
      "grad_norm": 1.5310360193252563,
      "learning_rate": 1.970033140373925e-05,
      "loss": 1.7049,
      "step": 2055
    },
    {
      "epoch": 1.7068876229932677,
      "grad_norm": 1.4705044031143188,
      "learning_rate": 1.959430815580268e-05,
      "loss": 1.7073,
      "step": 2060
    },
    {
      "epoch": 1.7110305541170376,
      "grad_norm": 1.5830848217010498,
      "learning_rate": 1.9488386802950076e-05,
      "loss": 1.6557,
      "step": 2065
    },
    {
      "epoch": 1.715173485240808,
      "grad_norm": 1.4873945713043213,
      "learning_rate": 1.938256934175571e-05,
      "loss": 1.6935,
      "step": 2070
    },
    {
      "epoch": 1.719316416364578,
      "grad_norm": 1.4743801355361938,
      "learning_rate": 1.9276857766835506e-05,
      "loss": 1.572,
      "step": 2075
    },
    {
      "epoch": 1.723459347488348,
      "grad_norm": 1.4847930669784546,
      "learning_rate": 1.9171254070809493e-05,
      "loss": 1.7382,
      "step": 2080
    },
    {
      "epoch": 1.7276022786121181,
      "grad_norm": 1.4579131603240967,
      "learning_rate": 1.906576024426422e-05,
      "loss": 1.6931,
      "step": 2085
    },
    {
      "epoch": 1.7317452097358883,
      "grad_norm": 1.3434035778045654,
      "learning_rate": 1.8960378275715234e-05,
      "loss": 1.7334,
      "step": 2090
    },
    {
      "epoch": 1.7358881408596583,
      "grad_norm": 1.4477676153182983,
      "learning_rate": 1.8855110151569633e-05,
      "loss": 1.6728,
      "step": 2095
    },
    {
      "epoch": 1.7400310719834282,
      "grad_norm": 1.355128288269043,
      "learning_rate": 1.8749957856088547e-05,
      "loss": 1.6807,
      "step": 2100
    },
    {
      "epoch": 1.7441740031071984,
      "grad_norm": 1.4280518293380737,
      "learning_rate": 1.864492337134982e-05,
      "loss": 1.7247,
      "step": 2105
    },
    {
      "epoch": 1.7483169342309686,
      "grad_norm": 1.3088176250457764,
      "learning_rate": 1.85400086772106e-05,
      "loss": 1.749,
      "step": 2110
    },
    {
      "epoch": 1.7524598653547385,
      "grad_norm": 1.405013918876648,
      "learning_rate": 1.8435215751270045e-05,
      "loss": 1.66,
      "step": 2115
    },
    {
      "epoch": 1.7566027964785085,
      "grad_norm": 1.3945555686950684,
      "learning_rate": 1.8330546568832002e-05,
      "loss": 1.671,
      "step": 2120
    },
    {
      "epoch": 1.7607457276022787,
      "grad_norm": 1.4923481941223145,
      "learning_rate": 1.822600310286782e-05,
      "loss": 1.6529,
      "step": 2125
    },
    {
      "epoch": 1.7648886587260488,
      "grad_norm": 1.4325562715530396,
      "learning_rate": 1.812158732397917e-05,
      "loss": 1.59,
      "step": 2130
    },
    {
      "epoch": 1.7690315898498188,
      "grad_norm": 1.605760931968689,
      "learning_rate": 1.8017301200360813e-05,
      "loss": 1.7221,
      "step": 2135
    },
    {
      "epoch": 1.7731745209735887,
      "grad_norm": 1.6106079816818237,
      "learning_rate": 1.7913146697763626e-05,
      "loss": 1.6433,
      "step": 2140
    },
    {
      "epoch": 1.777317452097359,
      "grad_norm": 1.6343430280685425,
      "learning_rate": 1.780912577945743e-05,
      "loss": 1.6857,
      "step": 2145
    },
    {
      "epoch": 1.781460383221129,
      "grad_norm": 1.380265235900879,
      "learning_rate": 1.7705240406194088e-05,
      "loss": 1.6521,
      "step": 2150
    },
    {
      "epoch": 1.785603314344899,
      "grad_norm": 1.4642555713653564,
      "learning_rate": 1.7601492536170438e-05,
      "loss": 1.6475,
      "step": 2155
    },
    {
      "epoch": 1.789746245468669,
      "grad_norm": 1.5754871368408203,
      "learning_rate": 1.749788412499149e-05,
      "loss": 1.6711,
      "step": 2160
    },
    {
      "epoch": 1.7938891765924392,
      "grad_norm": 1.41132390499115,
      "learning_rate": 1.7394417125633465e-05,
      "loss": 1.7038,
      "step": 2165
    },
    {
      "epoch": 1.7980321077162094,
      "grad_norm": 1.4552626609802246,
      "learning_rate": 1.7291093488407073e-05,
      "loss": 1.5968,
      "step": 2170
    },
    {
      "epoch": 1.8021750388399793,
      "grad_norm": 1.4994653463363647,
      "learning_rate": 1.718791516092069e-05,
      "loss": 1.7122,
      "step": 2175
    },
    {
      "epoch": 1.8063179699637493,
      "grad_norm": 1.5039201974868774,
      "learning_rate": 1.708488408804364e-05,
      "loss": 1.6803,
      "step": 2180
    },
    {
      "epoch": 1.8104609010875194,
      "grad_norm": 1.4056651592254639,
      "learning_rate": 1.698200221186959e-05,
      "loss": 1.6764,
      "step": 2185
    },
    {
      "epoch": 1.8146038322112896,
      "grad_norm": 1.3277151584625244,
      "learning_rate": 1.6879271471679885e-05,
      "loss": 1.6632,
      "step": 2190
    },
    {
      "epoch": 1.8187467633350596,
      "grad_norm": 1.40156090259552,
      "learning_rate": 1.6776693803907046e-05,
      "loss": 1.6913,
      "step": 2195
    },
    {
      "epoch": 1.8228896944588295,
      "grad_norm": 1.5100047588348389,
      "learning_rate": 1.6674271142098204e-05,
      "loss": 1.7645,
      "step": 2200
    },
    {
      "epoch": 1.8270326255825997,
      "grad_norm": 1.3332879543304443,
      "learning_rate": 1.6572005416878737e-05,
      "loss": 1.6632,
      "step": 2205
    },
    {
      "epoch": 1.83117555670637,
      "grad_norm": 1.4964687824249268,
      "learning_rate": 1.6469898555915785e-05,
      "loss": 1.6157,
      "step": 2210
    },
    {
      "epoch": 1.8353184878301398,
      "grad_norm": 1.606818675994873,
      "learning_rate": 1.6367952483881996e-05,
      "loss": 1.7074,
      "step": 2215
    },
    {
      "epoch": 1.8394614189539098,
      "grad_norm": 1.4448068141937256,
      "learning_rate": 1.6266169122419207e-05,
      "loss": 1.6711,
      "step": 2220
    },
    {
      "epoch": 1.84360435007768,
      "grad_norm": 1.4482852220535278,
      "learning_rate": 1.6164550390102207e-05,
      "loss": 1.69,
      "step": 2225
    },
    {
      "epoch": 1.8477472812014502,
      "grad_norm": 1.5581437349319458,
      "learning_rate": 1.6063098202402612e-05,
      "loss": 1.7297,
      "step": 2230
    },
    {
      "epoch": 1.85189021232522,
      "grad_norm": 1.379825234413147,
      "learning_rate": 1.596181447165273e-05,
      "loss": 1.6472,
      "step": 2235
    },
    {
      "epoch": 1.85603314344899,
      "grad_norm": 1.4702167510986328,
      "learning_rate": 1.586070110700955e-05,
      "loss": 1.6068,
      "step": 2240
    },
    {
      "epoch": 1.8601760745727602,
      "grad_norm": 1.5614300966262817,
      "learning_rate": 1.5759760014418678e-05,
      "loss": 1.6555,
      "step": 2245
    },
    {
      "epoch": 1.8643190056965304,
      "grad_norm": 1.5321334600448608,
      "learning_rate": 1.565899309657851e-05,
      "loss": 1.6616,
      "step": 2250
    },
    {
      "epoch": 1.8684619368203004,
      "grad_norm": 1.441321849822998,
      "learning_rate": 1.5558402252904298e-05,
      "loss": 1.6842,
      "step": 2255
    },
    {
      "epoch": 1.8726048679440703,
      "grad_norm": 1.4889140129089355,
      "learning_rate": 1.5457989379492366e-05,
      "loss": 1.6534,
      "step": 2260
    },
    {
      "epoch": 1.8767477990678405,
      "grad_norm": 1.5115433931350708,
      "learning_rate": 1.53577563690844e-05,
      "loss": 1.7116,
      "step": 2265
    },
    {
      "epoch": 1.8808907301916107,
      "grad_norm": 1.4987369775772095,
      "learning_rate": 1.5257705111031684e-05,
      "loss": 1.6615,
      "step": 2270
    },
    {
      "epoch": 1.8850336613153806,
      "grad_norm": 1.4837206602096558,
      "learning_rate": 1.5157837491259608e-05,
      "loss": 1.7218,
      "step": 2275
    },
    {
      "epoch": 1.8891765924391506,
      "grad_norm": 1.4923583269119263,
      "learning_rate": 1.5058155392232004e-05,
      "loss": 1.6915,
      "step": 2280
    },
    {
      "epoch": 1.8933195235629208,
      "grad_norm": 1.3571500778198242,
      "learning_rate": 1.4958660692915752e-05,
      "loss": 1.7102,
      "step": 2285
    },
    {
      "epoch": 1.897462454686691,
      "grad_norm": 1.4996232986450195,
      "learning_rate": 1.4859355268745294e-05,
      "loss": 1.6052,
      "step": 2290
    },
    {
      "epoch": 1.901605385810461,
      "grad_norm": 1.4513914585113525,
      "learning_rate": 1.4760240991587337e-05,
      "loss": 1.6644,
      "step": 2295
    },
    {
      "epoch": 1.9057483169342309,
      "grad_norm": 1.5274059772491455,
      "learning_rate": 1.4661319729705542e-05,
      "loss": 1.6603,
      "step": 2300
    },
    {
      "epoch": 1.909891248058001,
      "grad_norm": 1.496133804321289,
      "learning_rate": 1.4562593347725281e-05,
      "loss": 1.6634,
      "step": 2305
    },
    {
      "epoch": 1.9140341791817712,
      "grad_norm": 1.3887410163879395,
      "learning_rate": 1.4464063706598562e-05,
      "loss": 1.6821,
      "step": 2310
    },
    {
      "epoch": 1.9181771103055412,
      "grad_norm": 1.4953536987304688,
      "learning_rate": 1.4365732663568877e-05,
      "loss": 1.5945,
      "step": 2315
    },
    {
      "epoch": 1.9223200414293111,
      "grad_norm": 1.4123746156692505,
      "learning_rate": 1.426760207213624e-05,
      "loss": 1.6964,
      "step": 2320
    },
    {
      "epoch": 1.9264629725530813,
      "grad_norm": 1.5285396575927734,
      "learning_rate": 1.416967378202223e-05,
      "loss": 1.698,
      "step": 2325
    },
    {
      "epoch": 1.9306059036768515,
      "grad_norm": 1.4090371131896973,
      "learning_rate": 1.407194963913513e-05,
      "loss": 1.6563,
      "step": 2330
    },
    {
      "epoch": 1.9347488348006214,
      "grad_norm": 1.3992401361465454,
      "learning_rate": 1.3974431485535116e-05,
      "loss": 1.709,
      "step": 2335
    },
    {
      "epoch": 1.9388917659243914,
      "grad_norm": 1.5447429418563843,
      "learning_rate": 1.3877121159399587e-05,
      "loss": 1.6537,
      "step": 2340
    },
    {
      "epoch": 1.9430346970481616,
      "grad_norm": 1.4410513639450073,
      "learning_rate": 1.3780020494988446e-05,
      "loss": 1.6666,
      "step": 2345
    },
    {
      "epoch": 1.9471776281719317,
      "grad_norm": 1.5308438539505005,
      "learning_rate": 1.3683131322609572e-05,
      "loss": 1.6504,
      "step": 2350
    },
    {
      "epoch": 1.9513205592957017,
      "grad_norm": 1.4313169717788696,
      "learning_rate": 1.3586455468584291e-05,
      "loss": 1.7321,
      "step": 2355
    },
    {
      "epoch": 1.9554634904194717,
      "grad_norm": 1.3808836936950684,
      "learning_rate": 1.3489994755212992e-05,
      "loss": 1.6015,
      "step": 2360
    },
    {
      "epoch": 1.9596064215432418,
      "grad_norm": 1.3643015623092651,
      "learning_rate": 1.3393751000740723e-05,
      "loss": 1.714,
      "step": 2365
    },
    {
      "epoch": 1.963749352667012,
      "grad_norm": 1.530778408050537,
      "learning_rate": 1.3297726019322948e-05,
      "loss": 1.5797,
      "step": 2370
    },
    {
      "epoch": 1.967892283790782,
      "grad_norm": 1.3353676795959473,
      "learning_rate": 1.3201921620991393e-05,
      "loss": 1.6868,
      "step": 2375
    },
    {
      "epoch": 1.972035214914552,
      "grad_norm": 1.6299631595611572,
      "learning_rate": 1.31063396116198e-05,
      "loss": 1.5428,
      "step": 2380
    },
    {
      "epoch": 1.976178146038322,
      "grad_norm": 1.4689103364944458,
      "learning_rate": 1.3010981792890053e-05,
      "loss": 1.6567,
      "step": 2385
    },
    {
      "epoch": 1.9803210771620923,
      "grad_norm": 1.5441243648529053,
      "learning_rate": 1.2915849962258087e-05,
      "loss": 1.6254,
      "step": 2390
    },
    {
      "epoch": 1.9844640082858622,
      "grad_norm": 1.38924241065979,
      "learning_rate": 1.2820945912920051e-05,
      "loss": 1.6121,
      "step": 2395
    },
    {
      "epoch": 1.9886069394096322,
      "grad_norm": 1.6320685148239136,
      "learning_rate": 1.2726271433778558e-05,
      "loss": 1.6084,
      "step": 2400
    },
    {
      "epoch": 1.9927498705334024,
      "grad_norm": 1.5911461114883423,
      "learning_rate": 1.2631828309408838e-05,
      "loss": 1.7269,
      "step": 2405
    },
    {
      "epoch": 1.9968928016571725,
      "grad_norm": 1.4571669101715088,
      "learning_rate": 1.2537618320025266e-05,
      "loss": 1.6201,
      "step": 2410
    },
    {
      "epoch": 2.0010357327809425,
      "grad_norm": 1.4091567993164062,
      "learning_rate": 1.2443643241447627e-05,
      "loss": 1.6639,
      "step": 2415
    },
    {
      "epoch": 2.0051786639047124,
      "grad_norm": 1.3833612203598022,
      "learning_rate": 1.2349904845067817e-05,
      "loss": 1.637,
      "step": 2420
    },
    {
      "epoch": 2.009321595028483,
      "grad_norm": 1.4001517295837402,
      "learning_rate": 1.2256404897816317e-05,
      "loss": 1.6872,
      "step": 2425
    },
    {
      "epoch": 2.013464526152253,
      "grad_norm": 1.6197847127914429,
      "learning_rate": 1.2163145162128947e-05,
      "loss": 1.7429,
      "step": 2430
    },
    {
      "epoch": 2.0176074572760228,
      "grad_norm": 1.4687451124191284,
      "learning_rate": 1.2070127395913673e-05,
      "loss": 1.6477,
      "step": 2435
    },
    {
      "epoch": 2.0217503883997927,
      "grad_norm": 1.4636163711547852,
      "learning_rate": 1.1977353352517365e-05,
      "loss": 1.6643,
      "step": 2440
    },
    {
      "epoch": 2.025893319523563,
      "grad_norm": 1.3945302963256836,
      "learning_rate": 1.1884824780692899e-05,
      "loss": 1.7073,
      "step": 2445
    },
    {
      "epoch": 2.030036250647333,
      "grad_norm": 1.5510872602462769,
      "learning_rate": 1.1792543424566038e-05,
      "loss": 1.7629,
      "step": 2450
    },
    {
      "epoch": 2.034179181771103,
      "grad_norm": 1.4835284948349,
      "learning_rate": 1.170051102360269e-05,
      "loss": 1.6556,
      "step": 2455
    },
    {
      "epoch": 2.038322112894873,
      "grad_norm": 1.4855034351348877,
      "learning_rate": 1.1608729312576019e-05,
      "loss": 1.6302,
      "step": 2460
    },
    {
      "epoch": 2.0424650440186434,
      "grad_norm": 1.5551873445510864,
      "learning_rate": 1.151720002153381e-05,
      "loss": 1.6521,
      "step": 2465
    },
    {
      "epoch": 2.0466079751424133,
      "grad_norm": 1.4800223112106323,
      "learning_rate": 1.1425924875765824e-05,
      "loss": 1.6999,
      "step": 2470
    },
    {
      "epoch": 2.0507509062661833,
      "grad_norm": 1.51985502243042,
      "learning_rate": 1.1334905595771273e-05,
      "loss": 1.6307,
      "step": 2475
    },
    {
      "epoch": 2.0548938373899532,
      "grad_norm": 1.4919489622116089,
      "learning_rate": 1.1244143897226441e-05,
      "loss": 1.7012,
      "step": 2480
    },
    {
      "epoch": 2.0590367685137236,
      "grad_norm": 1.739404320716858,
      "learning_rate": 1.115364149095228e-05,
      "loss": 1.6357,
      "step": 2485
    },
    {
      "epoch": 2.0631796996374936,
      "grad_norm": 1.5610390901565552,
      "learning_rate": 1.1063400082882189e-05,
      "loss": 1.6825,
      "step": 2490
    },
    {
      "epoch": 2.0673226307612635,
      "grad_norm": 1.4220247268676758,
      "learning_rate": 1.0973421374029863e-05,
      "loss": 1.6948,
      "step": 2495
    },
    {
      "epoch": 2.0714655618850335,
      "grad_norm": 1.4091227054595947,
      "learning_rate": 1.0883707060457224e-05,
      "loss": 1.6768,
      "step": 2500
    },
    {
      "epoch": 2.075608493008804,
      "grad_norm": 1.4490989446640015,
      "learning_rate": 1.0794258833242452e-05,
      "loss": 1.6286,
      "step": 2505
    },
    {
      "epoch": 2.079751424132574,
      "grad_norm": 1.5466408729553223,
      "learning_rate": 1.0705078378448102e-05,
      "loss": 1.6945,
      "step": 2510
    },
    {
      "epoch": 2.083894355256344,
      "grad_norm": 1.5858041048049927,
      "learning_rate": 1.0616167377089325e-05,
      "loss": 1.6701,
      "step": 2515
    },
    {
      "epoch": 2.0880372863801138,
      "grad_norm": 1.576337456703186,
      "learning_rate": 1.0527527505102214e-05,
      "loss": 1.6071,
      "step": 2520
    },
    {
      "epoch": 2.092180217503884,
      "grad_norm": 1.7292176485061646,
      "learning_rate": 1.0439160433312148e-05,
      "loss": 1.5979,
      "step": 2525
    },
    {
      "epoch": 2.096323148627654,
      "grad_norm": 1.4066579341888428,
      "learning_rate": 1.0351067827402347e-05,
      "loss": 1.6564,
      "step": 2530
    },
    {
      "epoch": 2.100466079751424,
      "grad_norm": 1.5198190212249756,
      "learning_rate": 1.0263251347882465e-05,
      "loss": 1.674,
      "step": 2535
    },
    {
      "epoch": 2.104609010875194,
      "grad_norm": 1.5951024293899536,
      "learning_rate": 1.0175712650057285e-05,
      "loss": 1.6879,
      "step": 2540
    },
    {
      "epoch": 2.1087519419989644,
      "grad_norm": 1.519153356552124,
      "learning_rate": 1.008845338399551e-05,
      "loss": 1.714,
      "step": 2545
    },
    {
      "epoch": 2.1128948731227344,
      "grad_norm": 1.5573914051055908,
      "learning_rate": 1.0001475194498672e-05,
      "loss": 1.6205,
      "step": 2550
    },
    {
      "epoch": 2.1170378042465043,
      "grad_norm": 1.4278855323791504,
      "learning_rate": 9.914779721070152e-06,
      "loss": 1.7364,
      "step": 2555
    },
    {
      "epoch": 2.1211807353702743,
      "grad_norm": 1.4790068864822388,
      "learning_rate": 9.82836859788419e-06,
      "loss": 1.6081,
      "step": 2560
    },
    {
      "epoch": 2.1253236664940447,
      "grad_norm": 1.3646844625473022,
      "learning_rate": 9.742243453755202e-06,
      "loss": 1.6848,
      "step": 2565
    },
    {
      "epoch": 2.1294665976178146,
      "grad_norm": 1.5348769426345825,
      "learning_rate": 9.656405912106972e-06,
      "loss": 1.6682,
      "step": 2570
    },
    {
      "epoch": 2.1336095287415846,
      "grad_norm": 1.4749796390533447,
      "learning_rate": 9.57085759094211e-06,
      "loss": 1.682,
      "step": 2575
    },
    {
      "epoch": 2.1377524598653546,
      "grad_norm": 1.713908076286316,
      "learning_rate": 9.485600102811556e-06,
      "loss": 1.6648,
      "step": 2580
    },
    {
      "epoch": 2.141895390989125,
      "grad_norm": 1.4124904870986938,
      "learning_rate": 9.400635054784124e-06,
      "loss": 1.7116,
      "step": 2585
    },
    {
      "epoch": 2.146038322112895,
      "grad_norm": 1.4812126159667969,
      "learning_rate": 9.3159640484163e-06,
      "loss": 1.6076,
      "step": 2590
    },
    {
      "epoch": 2.150181253236665,
      "grad_norm": 1.513909101486206,
      "learning_rate": 9.231588679721957e-06,
      "loss": 1.688,
      "step": 2595
    },
    {
      "epoch": 2.154324184360435,
      "grad_norm": 1.463648796081543,
      "learning_rate": 9.147510539142357e-06,
      "loss": 1.6208,
      "step": 2600
    },
    {
      "epoch": 2.1584671154842052,
      "grad_norm": 1.5063291788101196,
      "learning_rate": 9.063731211516118e-06,
      "loss": 1.6606,
      "step": 2605
    },
    {
      "epoch": 2.162610046607975,
      "grad_norm": 1.3896359205245972,
      "learning_rate": 8.980252276049345e-06,
      "loss": 1.7367,
      "step": 2610
    },
    {
      "epoch": 2.166752977731745,
      "grad_norm": 1.5182971954345703,
      "learning_rate": 8.897075306285909e-06,
      "loss": 1.6629,
      "step": 2615
    },
    {
      "epoch": 2.170895908855515,
      "grad_norm": 1.4418666362762451,
      "learning_rate": 8.814201870077699e-06,
      "loss": 1.5967,
      "step": 2620
    },
    {
      "epoch": 2.1750388399792855,
      "grad_norm": 1.5458002090454102,
      "learning_rate": 8.731633529555166e-06,
      "loss": 1.7243,
      "step": 2625
    },
    {
      "epoch": 2.1791817711030554,
      "grad_norm": 1.7770825624465942,
      "learning_rate": 8.649371841097809e-06,
      "loss": 1.6837,
      "step": 2630
    },
    {
      "epoch": 2.1833247022268254,
      "grad_norm": 1.5128507614135742,
      "learning_rate": 8.567418355304866e-06,
      "loss": 1.7008,
      "step": 2635
    },
    {
      "epoch": 2.1874676333505954,
      "grad_norm": 1.500702977180481,
      "learning_rate": 8.48577461696608e-06,
      "loss": 1.6997,
      "step": 2640
    },
    {
      "epoch": 2.1916105644743658,
      "grad_norm": 1.4355021715164185,
      "learning_rate": 8.404442165032581e-06,
      "loss": 1.6307,
      "step": 2645
    },
    {
      "epoch": 2.1957534955981357,
      "grad_norm": 1.5311650037765503,
      "learning_rate": 8.323422532587879e-06,
      "loss": 1.7443,
      "step": 2650
    },
    {
      "epoch": 2.1998964267219057,
      "grad_norm": 1.5751802921295166,
      "learning_rate": 8.242717246818956e-06,
      "loss": 1.5734,
      "step": 2655
    },
    {
      "epoch": 2.2040393578456756,
      "grad_norm": 1.4290014505386353,
      "learning_rate": 8.162327828987507e-06,
      "loss": 1.7144,
      "step": 2660
    },
    {
      "epoch": 2.208182288969446,
      "grad_norm": 1.5428788661956787,
      "learning_rate": 8.082255794401231e-06,
      "loss": 1.6763,
      "step": 2665
    },
    {
      "epoch": 2.212325220093216,
      "grad_norm": 1.5341613292694092,
      "learning_rate": 8.002502652385276e-06,
      "loss": 1.7126,
      "step": 2670
    },
    {
      "epoch": 2.216468151216986,
      "grad_norm": 1.5137512683868408,
      "learning_rate": 7.92306990625381e-06,
      "loss": 1.8078,
      "step": 2675
    },
    {
      "epoch": 2.220611082340756,
      "grad_norm": 1.5194886922836304,
      "learning_rate": 7.843959053281663e-06,
      "loss": 1.6402,
      "step": 2680
    },
    {
      "epoch": 2.2247540134645263,
      "grad_norm": 1.6091029644012451,
      "learning_rate": 7.765171584676109e-06,
      "loss": 1.6453,
      "step": 2685
    },
    {
      "epoch": 2.2288969445882962,
      "grad_norm": 1.3683953285217285,
      "learning_rate": 7.686708985548762e-06,
      "loss": 1.563,
      "step": 2690
    },
    {
      "epoch": 2.233039875712066,
      "grad_norm": 1.3796041011810303,
      "learning_rate": 7.6085727348875676e-06,
      "loss": 1.7303,
      "step": 2695
    },
    {
      "epoch": 2.237182806835836,
      "grad_norm": 1.5532814264297485,
      "learning_rate": 7.530764305528959e-06,
      "loss": 1.6719,
      "step": 2700
    },
    {
      "epoch": 2.2413257379596065,
      "grad_norm": 1.5760571956634521,
      "learning_rate": 7.453285164130055e-06,
      "loss": 1.6119,
      "step": 2705
    },
    {
      "epoch": 2.2454686690833765,
      "grad_norm": 1.5009205341339111,
      "learning_rate": 7.3761367711410344e-06,
      "loss": 1.6483,
      "step": 2710
    },
    {
      "epoch": 2.2496116002071465,
      "grad_norm": 1.6418311595916748,
      "learning_rate": 7.299320580777599e-06,
      "loss": 1.6694,
      "step": 2715
    },
    {
      "epoch": 2.253754531330917,
      "grad_norm": 1.5269501209259033,
      "learning_rate": 7.222838040993565e-06,
      "loss": 1.659,
      "step": 2720
    },
    {
      "epoch": 2.257897462454687,
      "grad_norm": 1.3663263320922852,
      "learning_rate": 7.146690593453606e-06,
      "loss": 1.6832,
      "step": 2725
    },
    {
      "epoch": 2.2620403935784568,
      "grad_norm": 1.3697736263275146,
      "learning_rate": 7.070879673505976e-06,
      "loss": 1.6975,
      "step": 2730
    },
    {
      "epoch": 2.2661833247022267,
      "grad_norm": 1.5683199167251587,
      "learning_rate": 6.995406710155597e-06,
      "loss": 1.6984,
      "step": 2735
    },
    {
      "epoch": 2.2703262558259967,
      "grad_norm": 1.5766596794128418,
      "learning_rate": 6.920273126036978e-06,
      "loss": 1.6323,
      "step": 2740
    },
    {
      "epoch": 2.274469186949767,
      "grad_norm": 1.5342265367507935,
      "learning_rate": 6.8454803373875244e-06,
      "loss": 1.6684,
      "step": 2745
    },
    {
      "epoch": 2.278612118073537,
      "grad_norm": 1.4720062017440796,
      "learning_rate": 6.7710297540207525e-06,
      "loss": 1.6586,
      "step": 2750
    },
    {
      "epoch": 2.282755049197307,
      "grad_norm": 1.42441987991333,
      "learning_rate": 6.6969227792997556e-06,
      "loss": 1.6449,
      "step": 2755
    },
    {
      "epoch": 2.2868979803210774,
      "grad_norm": 1.424487590789795,
      "learning_rate": 6.623160810110765e-06,
      "loss": 1.6983,
      "step": 2760
    },
    {
      "epoch": 2.2910409114448473,
      "grad_norm": 1.3436846733093262,
      "learning_rate": 6.549745236836752e-06,
      "loss": 1.6912,
      "step": 2765
    },
    {
      "epoch": 2.2951838425686173,
      "grad_norm": 1.6133877038955688,
      "learning_rate": 6.476677443331328e-06,
      "loss": 1.6271,
      "step": 2770
    },
    {
      "epoch": 2.2993267736923872,
      "grad_norm": 1.2572236061096191,
      "learning_rate": 6.403958806892535e-06,
      "loss": 1.6471,
      "step": 2775
    },
    {
      "epoch": 2.303469704816157,
      "grad_norm": 1.594578504562378,
      "learning_rate": 6.331590698236997e-06,
      "loss": 1.6101,
      "step": 2780
    },
    {
      "epoch": 2.3076126359399276,
      "grad_norm": 1.4974867105484009,
      "learning_rate": 6.259574481474004e-06,
      "loss": 1.683,
      "step": 2785
    },
    {
      "epoch": 2.3117555670636976,
      "grad_norm": 1.5004031658172607,
      "learning_rate": 6.187911514079833e-06,
      "loss": 1.6171,
      "step": 2790
    },
    {
      "epoch": 2.3158984981874675,
      "grad_norm": 1.4604953527450562,
      "learning_rate": 6.1166031468721765e-06,
      "loss": 1.6969,
      "step": 2795
    },
    {
      "epoch": 2.320041429311238,
      "grad_norm": 1.5071390867233276,
      "learning_rate": 6.045650723984611e-06,
      "loss": 1.5547,
      "step": 2800
    },
    {
      "epoch": 2.324184360435008,
      "grad_norm": 1.46266770362854,
      "learning_rate": 5.975055582841358e-06,
      "loss": 1.6844,
      "step": 2805
    },
    {
      "epoch": 2.328327291558778,
      "grad_norm": 1.5470638275146484,
      "learning_rate": 5.904819054131994e-06,
      "loss": 1.5439,
      "step": 2810
    },
    {
      "epoch": 2.332470222682548,
      "grad_norm": 1.5609972476959229,
      "learning_rate": 5.834942461786408e-06,
      "loss": 1.6511,
      "step": 2815
    },
    {
      "epoch": 2.3366131538063177,
      "grad_norm": 1.4841610193252563,
      "learning_rate": 5.7654271229498295e-06,
      "loss": 1.7419,
      "step": 2820
    },
    {
      "epoch": 2.340756084930088,
      "grad_norm": 1.4989839792251587,
      "learning_rate": 5.696274347958008e-06,
      "loss": 1.6122,
      "step": 2825
    },
    {
      "epoch": 2.344899016053858,
      "grad_norm": 1.3839040994644165,
      "learning_rate": 5.627485440312533e-06,
      "loss": 1.6232,
      "step": 2830
    },
    {
      "epoch": 2.349041947177628,
      "grad_norm": 1.5615756511688232,
      "learning_rate": 5.559061696656198e-06,
      "loss": 1.651,
      "step": 2835
    },
    {
      "epoch": 2.3531848783013984,
      "grad_norm": 1.4670000076293945,
      "learning_rate": 5.491004406748654e-06,
      "loss": 1.6567,
      "step": 2840
    },
    {
      "epoch": 2.3573278094251684,
      "grad_norm": 1.7578529119491577,
      "learning_rate": 5.423314853442021e-06,
      "loss": 1.6801,
      "step": 2845
    },
    {
      "epoch": 2.3614707405489384,
      "grad_norm": 1.3822404146194458,
      "learning_rate": 5.355994312656734e-06,
      "loss": 1.6577,
      "step": 2850
    },
    {
      "epoch": 2.3656136716727083,
      "grad_norm": 1.6848986148834229,
      "learning_rate": 5.289044053357503e-06,
      "loss": 1.7138,
      "step": 2855
    },
    {
      "epoch": 2.3697566027964783,
      "grad_norm": 1.5494210720062256,
      "learning_rate": 5.222465337529376e-06,
      "loss": 1.671,
      "step": 2860
    },
    {
      "epoch": 2.3738995339202487,
      "grad_norm": 1.352903127670288,
      "learning_rate": 5.156259420153961e-06,
      "loss": 1.6783,
      "step": 2865
    },
    {
      "epoch": 2.3780424650440186,
      "grad_norm": 1.3841493129730225,
      "learning_rate": 5.090427549185758e-06,
      "loss": 1.5651,
      "step": 2870
    },
    {
      "epoch": 2.3821853961677886,
      "grad_norm": 1.5493215322494507,
      "learning_rate": 5.024970965528672e-06,
      "loss": 1.6571,
      "step": 2875
    },
    {
      "epoch": 2.386328327291559,
      "grad_norm": 1.4074615240097046,
      "learning_rate": 4.959890903012568e-06,
      "loss": 1.6348,
      "step": 2880
    },
    {
      "epoch": 2.390471258415329,
      "grad_norm": 1.3831806182861328,
      "learning_rate": 4.895188588370051e-06,
      "loss": 1.658,
      "step": 2885
    },
    {
      "epoch": 2.394614189539099,
      "grad_norm": 1.6874393224716187,
      "learning_rate": 4.8308652412133305e-06,
      "loss": 1.6397,
      "step": 2890
    },
    {
      "epoch": 2.398757120662869,
      "grad_norm": 1.5508131980895996,
      "learning_rate": 4.766922074011238e-06,
      "loss": 1.7173,
      "step": 2895
    },
    {
      "epoch": 2.4029000517866392,
      "grad_norm": 1.3305059671401978,
      "learning_rate": 4.703360292066358e-06,
      "loss": 1.6444,
      "step": 2900
    },
    {
      "epoch": 2.407042982910409,
      "grad_norm": 1.454041600227356,
      "learning_rate": 4.640181093492346e-06,
      "loss": 1.6754,
      "step": 2905
    },
    {
      "epoch": 2.411185914034179,
      "grad_norm": 1.5702210664749146,
      "learning_rate": 4.577385669191273e-06,
      "loss": 1.6119,
      "step": 2910
    },
    {
      "epoch": 2.415328845157949,
      "grad_norm": 1.43020498752594,
      "learning_rate": 4.514975202831262e-06,
      "loss": 1.5034,
      "step": 2915
    },
    {
      "epoch": 2.4194717762817195,
      "grad_norm": 1.4899375438690186,
      "learning_rate": 4.452950870824116e-06,
      "loss": 1.6711,
      "step": 2920
    },
    {
      "epoch": 2.4236147074054895,
      "grad_norm": 1.5223251581192017,
      "learning_rate": 4.391313842303166e-06,
      "loss": 1.581,
      "step": 2925
    },
    {
      "epoch": 2.4277576385292594,
      "grad_norm": 1.6889452934265137,
      "learning_rate": 4.330065279101233e-06,
      "loss": 1.6475,
      "step": 2930
    },
    {
      "epoch": 2.4319005696530294,
      "grad_norm": 1.5639691352844238,
      "learning_rate": 4.269206335728721e-06,
      "loss": 1.676,
      "step": 2935
    },
    {
      "epoch": 2.4360435007767998,
      "grad_norm": 1.5441943407058716,
      "learning_rate": 4.208738159351872e-06,
      "loss": 1.6674,
      "step": 2940
    },
    {
      "epoch": 2.4401864319005697,
      "grad_norm": 1.4562424421310425,
      "learning_rate": 4.148661889771102e-06,
      "loss": 1.559,
      "step": 2945
    },
    {
      "epoch": 2.4443293630243397,
      "grad_norm": 1.6757646799087524,
      "learning_rate": 4.088978659399581e-06,
      "loss": 1.5979,
      "step": 2950
    },
    {
      "epoch": 2.4484722941481096,
      "grad_norm": 1.6473145484924316,
      "learning_rate": 4.02968959324182e-06,
      "loss": 1.7553,
      "step": 2955
    },
    {
      "epoch": 2.45261522527188,
      "grad_norm": 1.509401798248291,
      "learning_rate": 3.97079580887251e-06,
      "loss": 1.6136,
      "step": 2960
    },
    {
      "epoch": 2.45675815639565,
      "grad_norm": 1.684385061264038,
      "learning_rate": 3.912298416415442e-06,
      "loss": 1.6744,
      "step": 2965
    },
    {
      "epoch": 2.46090108751942,
      "grad_norm": 1.4679051637649536,
      "learning_rate": 3.8541985185225645e-06,
      "loss": 1.6135,
      "step": 2970
    },
    {
      "epoch": 2.46504401864319,
      "grad_norm": 1.4584479331970215,
      "learning_rate": 3.7964972103532503e-06,
      "loss": 1.6634,
      "step": 2975
    },
    {
      "epoch": 2.4691869497669603,
      "grad_norm": 1.5034840106964111,
      "learning_rate": 3.7391955795535753e-06,
      "loss": 1.6484,
      "step": 2980
    },
    {
      "epoch": 2.4733298808907302,
      "grad_norm": 1.6042097806930542,
      "learning_rate": 3.6822947062359003e-06,
      "loss": 1.6706,
      "step": 2985
    },
    {
      "epoch": 2.4774728120145,
      "grad_norm": 1.4411003589630127,
      "learning_rate": 3.625795662958445e-06,
      "loss": 1.644,
      "step": 2990
    },
    {
      "epoch": 2.48161574313827,
      "grad_norm": 2.152674674987793,
      "learning_rate": 3.569699514705105e-06,
      "loss": 1.6202,
      "step": 2995
    },
    {
      "epoch": 2.4857586742620406,
      "grad_norm": 1.5934232473373413,
      "learning_rate": 3.5140073188653697e-06,
      "loss": 1.6052,
      "step": 3000
    },
    {
      "epoch": 2.4899016053858105,
      "grad_norm": 1.4413870573043823,
      "learning_rate": 3.4587201252143875e-06,
      "loss": 1.7475,
      "step": 3005
    },
    {
      "epoch": 2.4940445365095805,
      "grad_norm": 1.385025978088379,
      "learning_rate": 3.4038389758931975e-06,
      "loss": 1.5893,
      "step": 3010
    },
    {
      "epoch": 2.4981874676333504,
      "grad_norm": 1.5067813396453857,
      "learning_rate": 3.3493649053890326e-06,
      "loss": 1.6509,
      "step": 3015
    },
    {
      "epoch": 2.502330398757121,
      "grad_norm": 1.5839219093322754,
      "learning_rate": 3.295298940515898e-06,
      "loss": 1.712,
      "step": 3020
    },
    {
      "epoch": 2.5064733298808908,
      "grad_norm": 1.388162612915039,
      "learning_rate": 3.2416421003951454e-06,
      "loss": 1.6229,
      "step": 3025
    },
    {
      "epoch": 2.5106162610046607,
      "grad_norm": 1.4393280744552612,
      "learning_rate": 3.1883953964363055e-06,
      "loss": 1.6738,
      "step": 3030
    },
    {
      "epoch": 2.514759192128431,
      "grad_norm": 1.4308174848556519,
      "learning_rate": 3.1355598323180025e-06,
      "loss": 1.6221,
      "step": 3035
    },
    {
      "epoch": 2.518902123252201,
      "grad_norm": 1.5354194641113281,
      "learning_rate": 3.0831364039690474e-06,
      "loss": 1.691,
      "step": 3040
    },
    {
      "epoch": 2.523045054375971,
      "grad_norm": 1.3682780265808105,
      "learning_rate": 3.031126099549653e-06,
      "loss": 1.8182,
      "step": 3045
    },
    {
      "epoch": 2.527187985499741,
      "grad_norm": 1.6484651565551758,
      "learning_rate": 2.979529899432834e-06,
      "loss": 1.701,
      "step": 3050
    },
    {
      "epoch": 2.531330916623511,
      "grad_norm": 1.5502278804779053,
      "learning_rate": 2.928348776185885e-06,
      "loss": 1.5731,
      "step": 3055
    },
    {
      "epoch": 2.5354738477472814,
      "grad_norm": 1.6697767972946167,
      "learning_rate": 2.877583694552083e-06,
      "loss": 1.6225,
      "step": 3060
    },
    {
      "epoch": 2.5396167788710513,
      "grad_norm": 1.5842533111572266,
      "learning_rate": 2.827235611432488e-06,
      "loss": 1.6301,
      "step": 3065
    },
    {
      "epoch": 2.5437597099948213,
      "grad_norm": 1.4051748514175415,
      "learning_rate": 2.7773054758679086e-06,
      "loss": 1.6346,
      "step": 3070
    },
    {
      "epoch": 2.5479026411185917,
      "grad_norm": 1.468464970588684,
      "learning_rate": 2.7277942290210107e-06,
      "loss": 1.6494,
      "step": 3075
    },
    {
      "epoch": 2.5520455722423616,
      "grad_norm": 1.3562371730804443,
      "learning_rate": 2.6787028041585784e-06,
      "loss": 1.7153,
      "step": 3080
    },
    {
      "epoch": 2.5561885033661316,
      "grad_norm": 1.6196010112762451,
      "learning_rate": 2.630032126633938e-06,
      "loss": 1.7219,
      "step": 3085
    },
    {
      "epoch": 2.5603314344899015,
      "grad_norm": 1.6541948318481445,
      "learning_rate": 2.581783113869468e-06,
      "loss": 1.657,
      "step": 3090
    },
    {
      "epoch": 2.5644743656136715,
      "grad_norm": 1.4300472736358643,
      "learning_rate": 2.5339566753393717e-06,
      "loss": 1.7434,
      "step": 3095
    },
    {
      "epoch": 2.568617296737442,
      "grad_norm": 1.6979039907455444,
      "learning_rate": 2.486553712552481e-06,
      "loss": 1.5847,
      "step": 3100
    },
    {
      "epoch": 2.572760227861212,
      "grad_norm": 1.4978907108306885,
      "learning_rate": 2.4395751190352924e-06,
      "loss": 1.6557,
      "step": 3105
    },
    {
      "epoch": 2.576903158984982,
      "grad_norm": 1.5635597705841064,
      "learning_rate": 2.3930217803151077e-06,
      "loss": 1.7336,
      "step": 3110
    },
    {
      "epoch": 2.581046090108752,
      "grad_norm": 1.4630043506622314,
      "learning_rate": 2.3468945739033492e-06,
      "loss": 1.5949,
      "step": 3115
    },
    {
      "epoch": 2.585189021232522,
      "grad_norm": 1.5605254173278809,
      "learning_rate": 2.301194369279039e-06,
      "loss": 1.6919,
      "step": 3120
    },
    {
      "epoch": 2.589331952356292,
      "grad_norm": 1.4356878995895386,
      "learning_rate": 2.255922027872351e-06,
      "loss": 1.6714,
      "step": 3125
    },
    {
      "epoch": 2.593474883480062,
      "grad_norm": 1.4694339036941528,
      "learning_rate": 2.211078403048453e-06,
      "loss": 1.561,
      "step": 3130
    },
    {
      "epoch": 2.597617814603832,
      "grad_norm": 1.4556609392166138,
      "learning_rate": 2.166664340091351e-06,
      "loss": 1.6505,
      "step": 3135
    },
    {
      "epoch": 2.6017607457276024,
      "grad_norm": 1.449992060661316,
      "learning_rate": 2.122680676187999e-06,
      "loss": 1.6515,
      "step": 3140
    },
    {
      "epoch": 2.6059036768513724,
      "grad_norm": 1.6447935104370117,
      "learning_rate": 2.079128240412512e-06,
      "loss": 1.7499,
      "step": 3145
    },
    {
      "epoch": 2.6100466079751423,
      "grad_norm": 1.457542061805725,
      "learning_rate": 2.036007853710503e-06,
      "loss": 1.6944,
      "step": 3150
    },
    {
      "epoch": 2.6141895390989127,
      "grad_norm": 1.410488486289978,
      "learning_rate": 1.9933203288836744e-06,
      "loss": 1.7327,
      "step": 3155
    },
    {
      "epoch": 2.6183324702226827,
      "grad_norm": 1.4986528158187866,
      "learning_rate": 1.951066470574417e-06,
      "loss": 1.6566,
      "step": 3160
    },
    {
      "epoch": 2.6224754013464526,
      "grad_norm": 1.348462462425232,
      "learning_rate": 1.9092470752507222e-06,
      "loss": 1.6556,
      "step": 3165
    },
    {
      "epoch": 2.6266183324702226,
      "grad_norm": 1.6997922658920288,
      "learning_rate": 1.867862931191111e-06,
      "loss": 1.6537,
      "step": 3170
    },
    {
      "epoch": 2.6307612635939925,
      "grad_norm": 1.4287980794906616,
      "learning_rate": 1.826914818469802e-06,
      "loss": 1.7428,
      "step": 3175
    },
    {
      "epoch": 2.634904194717763,
      "grad_norm": 1.4143716096878052,
      "learning_rate": 1.7864035089419972e-06,
      "loss": 1.6457,
      "step": 3180
    },
    {
      "epoch": 2.639047125841533,
      "grad_norm": 1.475485920906067,
      "learning_rate": 1.7463297662293377e-06,
      "loss": 1.7862,
      "step": 3185
    },
    {
      "epoch": 2.643190056965303,
      "grad_norm": 1.5003294944763184,
      "learning_rate": 1.70669434570552e-06,
      "loss": 1.6593,
      "step": 3190
    },
    {
      "epoch": 2.6473329880890732,
      "grad_norm": 1.4816300868988037,
      "learning_rate": 1.6674979944820257e-06,
      "loss": 1.7215,
      "step": 3195
    },
    {
      "epoch": 2.651475919212843,
      "grad_norm": 1.7278501987457275,
      "learning_rate": 1.628741451394078e-06,
      "loss": 1.6236,
      "step": 3200
    },
    {
      "epoch": 2.655618850336613,
      "grad_norm": 1.5147420167922974,
      "learning_rate": 1.5904254469866848e-06,
      "loss": 1.7254,
      "step": 3205
    },
    {
      "epoch": 2.659761781460383,
      "grad_norm": 1.464715600013733,
      "learning_rate": 1.552550703500885e-06,
      "loss": 1.6401,
      "step": 3210
    },
    {
      "epoch": 2.663904712584153,
      "grad_norm": 1.7878389358520508,
      "learning_rate": 1.5151179348601301e-06,
      "loss": 1.6898,
      "step": 3215
    },
    {
      "epoch": 2.6680476437079235,
      "grad_norm": 1.5522373914718628,
      "learning_rate": 1.478127846656821e-06,
      "loss": 1.6528,
      "step": 3220
    },
    {
      "epoch": 2.6721905748316934,
      "grad_norm": 1.509292721748352,
      "learning_rate": 1.441581136139014e-06,
      "loss": 1.7488,
      "step": 3225
    },
    {
      "epoch": 2.6763335059554634,
      "grad_norm": 1.4407262802124023,
      "learning_rate": 1.4054784921972874e-06,
      "loss": 1.6839,
      "step": 3230
    },
    {
      "epoch": 2.6804764370792338,
      "grad_norm": 1.5387518405914307,
      "learning_rate": 1.369820595351734e-06,
      "loss": 1.649,
      "step": 3235
    },
    {
      "epoch": 2.6846193682030037,
      "grad_norm": 1.3021140098571777,
      "learning_rate": 1.3346081177391472e-06,
      "loss": 1.6629,
      "step": 3240
    },
    {
      "epoch": 2.6887622993267737,
      "grad_norm": 1.494094967842102,
      "learning_rate": 1.2998417231003534e-06,
      "loss": 1.6438,
      "step": 3245
    },
    {
      "epoch": 2.6929052304505436,
      "grad_norm": 1.4297062158584595,
      "learning_rate": 1.2655220667676964e-06,
      "loss": 1.6758,
      "step": 3250
    },
    {
      "epoch": 2.6970481615743136,
      "grad_norm": 1.5370938777923584,
      "learning_rate": 1.231649795652684e-06,
      "loss": 1.6552,
      "step": 3255
    },
    {
      "epoch": 2.701191092698084,
      "grad_norm": 1.3752963542938232,
      "learning_rate": 1.1982255482337918e-06,
      "loss": 1.5742,
      "step": 3260
    },
    {
      "epoch": 2.705334023821854,
      "grad_norm": 1.4810984134674072,
      "learning_rate": 1.165249954544445e-06,
      "loss": 1.7383,
      "step": 3265
    },
    {
      "epoch": 2.709476954945624,
      "grad_norm": 1.7504469156265259,
      "learning_rate": 1.1327236361611065e-06,
      "loss": 1.6467,
      "step": 3270
    },
    {
      "epoch": 2.7136198860693943,
      "grad_norm": 1.5077506303787231,
      "learning_rate": 1.10064720619161e-06,
      "loss": 1.6392,
      "step": 3275
    },
    {
      "epoch": 2.7177628171931643,
      "grad_norm": 1.6127705574035645,
      "learning_rate": 1.0690212692635564e-06,
      "loss": 1.6938,
      "step": 3280
    },
    {
      "epoch": 2.721905748316934,
      "grad_norm": 1.39089834690094,
      "learning_rate": 1.037846421512942e-06,
      "loss": 1.6968,
      "step": 3285
    },
    {
      "epoch": 2.726048679440704,
      "grad_norm": 1.330757737159729,
      "learning_rate": 1.0071232505729194e-06,
      "loss": 1.6603,
      "step": 3290
    },
    {
      "epoch": 2.730191610564474,
      "grad_norm": 1.500431776046753,
      "learning_rate": 9.76852335562714e-07,
      "loss": 1.672,
      "step": 3295
    },
    {
      "epoch": 2.7343345416882445,
      "grad_norm": 1.5062246322631836,
      "learning_rate": 9.470342470767196e-07,
      "loss": 1.6577,
      "step": 3300
    },
    {
      "epoch": 2.7384774728120145,
      "grad_norm": 1.6007041931152344,
      "learning_rate": 9.17669547173719e-07,
      "loss": 1.5944,
      "step": 3305
    },
    {
      "epoch": 2.7426204039357844,
      "grad_norm": 1.3901289701461792,
      "learning_rate": 8.887587893663202e-07,
      "loss": 1.7273,
      "step": 3310
    },
    {
      "epoch": 2.746763335059555,
      "grad_norm": 1.5428518056869507,
      "learning_rate": 8.603025186105063e-07,
      "loss": 1.729,
      "step": 3315
    },
    {
      "epoch": 2.750906266183325,
      "grad_norm": 1.4763684272766113,
      "learning_rate": 8.32301271295352e-07,
      "loss": 1.6237,
      "step": 3320
    },
    {
      "epoch": 2.7550491973070947,
      "grad_norm": 1.4913324117660522,
      "learning_rate": 8.047555752329489e-07,
      "loss": 1.7174,
      "step": 3325
    },
    {
      "epoch": 2.7591921284308647,
      "grad_norm": 1.4215017557144165,
      "learning_rate": 7.77665949648404e-07,
      "loss": 1.5995,
      "step": 3330
    },
    {
      "epoch": 2.7633350595546347,
      "grad_norm": 1.6363754272460938,
      "learning_rate": 7.510329051701154e-07,
      "loss": 1.6775,
      "step": 3335
    },
    {
      "epoch": 2.767477990678405,
      "grad_norm": 1.4629477262496948,
      "learning_rate": 7.24856943820082e-07,
      "loss": 1.7798,
      "step": 3340
    },
    {
      "epoch": 2.771620921802175,
      "grad_norm": 1.4978421926498413,
      "learning_rate": 6.991385590044947e-07,
      "loss": 1.7056,
      "step": 3345
    },
    {
      "epoch": 2.775763852925945,
      "grad_norm": 1.416703701019287,
      "learning_rate": 6.738782355044049e-07,
      "loss": 1.7224,
      "step": 3350
    },
    {
      "epoch": 2.7799067840497154,
      "grad_norm": 1.4643096923828125,
      "learning_rate": 6.490764494665929e-07,
      "loss": 1.7447,
      "step": 3355
    },
    {
      "epoch": 2.7840497151734853,
      "grad_norm": 1.7302265167236328,
      "learning_rate": 6.247336683946031e-07,
      "loss": 1.6587,
      "step": 3360
    },
    {
      "epoch": 2.7881926462972553,
      "grad_norm": 1.5500733852386475,
      "learning_rate": 6.008503511399144e-07,
      "loss": 1.7087,
      "step": 3365
    },
    {
      "epoch": 2.7923355774210252,
      "grad_norm": 1.6820688247680664,
      "learning_rate": 5.774269478933114e-07,
      "loss": 1.6215,
      "step": 3370
    },
    {
      "epoch": 2.796478508544795,
      "grad_norm": 1.5368272066116333,
      "learning_rate": 5.544639001763718e-07,
      "loss": 1.6109,
      "step": 3375
    },
    {
      "epoch": 2.8006214396685656,
      "grad_norm": 1.4004430770874023,
      "learning_rate": 5.319616408331757e-07,
      "loss": 1.6692,
      "step": 3380
    },
    {
      "epoch": 2.8047643707923355,
      "grad_norm": 1.4603769779205322,
      "learning_rate": 5.099205940221202e-07,
      "loss": 1.701,
      "step": 3385
    },
    {
      "epoch": 2.8089073019161055,
      "grad_norm": 1.458081603050232,
      "learning_rate": 4.883411752079375e-07,
      "loss": 1.6285,
      "step": 3390
    },
    {
      "epoch": 2.813050233039876,
      "grad_norm": 1.593636393547058,
      "learning_rate": 4.672237911538646e-07,
      "loss": 1.6482,
      "step": 3395
    },
    {
      "epoch": 2.817193164163646,
      "grad_norm": 1.529858112335205,
      "learning_rate": 4.465688399139606e-07,
      "loss": 1.6119,
      "step": 3400
    },
    {
      "epoch": 2.821336095287416,
      "grad_norm": 1.5602949857711792,
      "learning_rate": 4.2637671082563225e-07,
      "loss": 1.5977,
      "step": 3405
    },
    {
      "epoch": 2.8254790264111858,
      "grad_norm": 1.5839064121246338,
      "learning_rate": 4.06647784502262e-07,
      "loss": 1.5513,
      "step": 3410
    },
    {
      "epoch": 2.8296219575349557,
      "grad_norm": 1.6716989278793335,
      "learning_rate": 3.873824328260556e-07,
      "loss": 1.6445,
      "step": 3415
    },
    {
      "epoch": 2.833764888658726,
      "grad_norm": 1.5342715978622437,
      "learning_rate": 3.685810189410277e-07,
      "loss": 1.689,
      "step": 3420
    },
    {
      "epoch": 2.837907819782496,
      "grad_norm": 1.6327084302902222,
      "learning_rate": 3.5024389724614967e-07,
      "loss": 1.6442,
      "step": 3425
    },
    {
      "epoch": 2.842050750906266,
      "grad_norm": 1.5646637678146362,
      "learning_rate": 3.323714133886796e-07,
      "loss": 1.6384,
      "step": 3430
    },
    {
      "epoch": 2.8461936820300364,
      "grad_norm": 1.5476573705673218,
      "learning_rate": 3.149639042576424e-07,
      "loss": 1.6424,
      "step": 3435
    },
    {
      "epoch": 2.8503366131538064,
      "grad_norm": 1.4045779705047607,
      "learning_rate": 2.980216979774714e-07,
      "loss": 1.7167,
      "step": 3440
    },
    {
      "epoch": 2.8544795442775763,
      "grad_norm": 1.528001308441162,
      "learning_rate": 2.81545113901846e-07,
      "loss": 1.662,
      "step": 3445
    },
    {
      "epoch": 2.8586224754013463,
      "grad_norm": 1.618828296661377,
      "learning_rate": 2.655344626076417e-07,
      "loss": 1.7386,
      "step": 3450
    },
    {
      "epoch": 2.8627654065251167,
      "grad_norm": 1.5477008819580078,
      "learning_rate": 2.49990045889098e-07,
      "loss": 1.6217,
      "step": 3455
    },
    {
      "epoch": 2.8669083376488866,
      "grad_norm": 1.5536863803863525,
      "learning_rate": 2.3491215675212618e-07,
      "loss": 1.6374,
      "step": 3460
    },
    {
      "epoch": 2.8710512687726566,
      "grad_norm": 1.603933572769165,
      "learning_rate": 2.203010794087773e-07,
      "loss": 1.7192,
      "step": 3465
    },
    {
      "epoch": 2.875194199896427,
      "grad_norm": 1.458042860031128,
      "learning_rate": 2.0615708927189113e-07,
      "loss": 1.6583,
      "step": 3470
    },
    {
      "epoch": 2.879337131020197,
      "grad_norm": 1.5184723138809204,
      "learning_rate": 1.9248045294991136e-07,
      "loss": 1.6954,
      "step": 3475
    },
    {
      "epoch": 2.883480062143967,
      "grad_norm": 1.393036961555481,
      "learning_rate": 1.7927142824184785e-07,
      "loss": 1.6954,
      "step": 3480
    },
    {
      "epoch": 2.887622993267737,
      "grad_norm": 1.6001871824264526,
      "learning_rate": 1.665302641324168e-07,
      "loss": 1.6656,
      "step": 3485
    },
    {
      "epoch": 2.891765924391507,
      "grad_norm": 1.5499097108840942,
      "learning_rate": 1.5425720078736383e-07,
      "loss": 1.6147,
      "step": 3490
    },
    {
      "epoch": 2.895908855515277,
      "grad_norm": 1.534512996673584,
      "learning_rate": 1.4245246954892321e-07,
      "loss": 1.7466,
      "step": 3495
    },
    {
      "epoch": 2.900051786639047,
      "grad_norm": 1.6099002361297607,
      "learning_rate": 1.311162929314519e-07,
      "loss": 1.6355,
      "step": 3500
    },
    {
      "epoch": 2.904194717762817,
      "grad_norm": 1.4561357498168945,
      "learning_rate": 1.2024888461726058e-07,
      "loss": 1.7213,
      "step": 3505
    },
    {
      "epoch": 2.9083376488865875,
      "grad_norm": 1.420392394065857,
      "learning_rate": 1.0985044945254764e-07,
      "loss": 1.6259,
      "step": 3510
    },
    {
      "epoch": 2.9124805800103575,
      "grad_norm": 1.4631907939910889,
      "learning_rate": 9.992118344357703e-08,
      "loss": 1.5965,
      "step": 3515
    },
    {
      "epoch": 2.9166235111341274,
      "grad_norm": 1.4051051139831543,
      "learning_rate": 9.046127375295643e-08,
      "loss": 1.6985,
      "step": 3520
    },
    {
      "epoch": 2.9207664422578974,
      "grad_norm": 1.4364763498306274,
      "learning_rate": 8.147089869612045e-08,
      "loss": 1.6201,
      "step": 3525
    },
    {
      "epoch": 2.9249093733816673,
      "grad_norm": 1.470716953277588,
      "learning_rate": 7.295022773796678e-08,
      "loss": 1.7103,
      "step": 3530
    },
    {
      "epoch": 2.9290523045054377,
      "grad_norm": 1.6881535053253174,
      "learning_rate": 6.489942148966699e-08,
      "loss": 1.5645,
      "step": 3535
    },
    {
      "epoch": 2.9331952356292077,
      "grad_norm": 1.5829025506973267,
      "learning_rate": 5.7318631705630124e-08,
      "loss": 1.6527,
      "step": 3540
    },
    {
      "epoch": 2.9373381667529777,
      "grad_norm": 1.5447124242782593,
      "learning_rate": 5.0208001280646625e-08,
      "loss": 1.6729,
      "step": 3545
    },
    {
      "epoch": 2.941481097876748,
      "grad_norm": 1.6347018480300903,
      "learning_rate": 4.3567664247198825e-08,
      "loss": 1.6677,
      "step": 3550
    },
    {
      "epoch": 2.945624029000518,
      "grad_norm": 1.4936102628707886,
      "learning_rate": 3.739774577292687e-08,
      "loss": 1.6392,
      "step": 3555
    },
    {
      "epoch": 2.949766960124288,
      "grad_norm": 1.5562516450881958,
      "learning_rate": 3.1698362158277814e-08,
      "loss": 1.5919,
      "step": 3560
    },
    {
      "epoch": 2.953909891248058,
      "grad_norm": 1.4463692903518677,
      "learning_rate": 2.6469620834299047e-08,
      "loss": 1.7659,
      "step": 3565
    },
    {
      "epoch": 2.958052822371828,
      "grad_norm": 1.5193216800689697,
      "learning_rate": 2.1711620360634343e-08,
      "loss": 1.6178,
      "step": 3570
    },
    {
      "epoch": 2.9621957534955983,
      "grad_norm": 1.4271697998046875,
      "learning_rate": 1.7424450423650373e-08,
      "loss": 1.5781,
      "step": 3575
    },
    {
      "epoch": 2.9663386846193682,
      "grad_norm": 1.4125490188598633,
      "learning_rate": 1.3608191834746375e-08,
      "loss": 1.6756,
      "step": 3580
    },
    {
      "epoch": 2.970481615743138,
      "grad_norm": 1.5000301599502563,
      "learning_rate": 1.0262916528841482e-08,
      "loss": 1.5881,
      "step": 3585
    },
    {
      "epoch": 2.9746245468669086,
      "grad_norm": 1.4616612195968628,
      "learning_rate": 7.3886875630091445e-09,
      "loss": 1.7884,
      "step": 3590
    },
    {
      "epoch": 2.9787674779906785,
      "grad_norm": 1.3853660821914673,
      "learning_rate": 4.985559115289195e-09,
      "loss": 1.6537,
      "step": 3595
    },
    {
      "epoch": 2.9829104091144485,
      "grad_norm": 1.4552274942398071,
      "learning_rate": 3.0535764836747695e-09,
      "loss": 1.7756,
      "step": 3600
    }
  ],
  "logging_steps": 5,
  "max_steps": 3618,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 100,
  "total_flos": 6.659884215508009e+17,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
